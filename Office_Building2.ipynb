{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmad162539/Smartbuilding/blob/main/Office_Building2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GftuNUW6dBmo"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BBirQUCPdBmp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url_office= 'https://raw.githubusercontent.com/Ahmad162539/Smartbuilding/main/data%20of%20Office_Abigail.csv'\n",
        "office= pd.read_csv(url_office)\n",
        "data_office= office[(office['timestamp'] > '2015-03-01') & (office['timestamp'] < '2015-06-01')]\n",
        "dfs_office=data_office['energy']\n",
        "datas_office=pd.DataFrame(dfs_office)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUFQ9-2TdBmq"
      },
      "source": [
        "## import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K6WCblUDdBmq"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import warnings\n",
        "\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter('ignore')\n",
        "\n",
        "import numpy\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn import metrics\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install EMD-signal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQaC-t3bdTxv",
        "outputId": "9cb92b0f-3409-4fe9-d628-ec31c3357d44"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting EMD-signal\n",
            "  Downloading EMD_signal-1.5.1-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.10/dist-packages (from EMD-signal) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from EMD-signal) (1.10.1)\n",
            "Collecting pathos>=0.2.1 (from EMD-signal)\n",
            "  Downloading pathos-0.3.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.64.* (from EMD-signal)\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ppft>=1.7.6.7 (from pathos>=0.2.1->EMD-signal)\n",
            "  Downloading ppft-1.7.6.7-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill>=0.3.7 (from pathos>=0.2.1->EMD-signal)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.3 (from pathos>=0.2.1->EMD-signal)\n",
            "  Downloading pox-0.3.3-py3-none-any.whl (29 kB)\n",
            "Collecting multiprocess>=0.70.15 (from pathos>=0.2.1->EMD-signal)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm, ppft, pox, dill, multiprocess, pathos, EMD-signal\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.0\n",
            "    Uninstalling tqdm-4.66.0:\n",
            "      Successfully uninstalled tqdm-4.66.0\n",
            "Successfully installed EMD-signal-1.5.1 dill-0.3.7 multiprocess-0.70.15 pathos-0.3.1 pox-0.3.3 ppft-1.7.6.7 tqdm-4.64.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyEMD import CEEMDAN"
      ],
      "metadata": {
        "id": "GybeVDjldaW2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dataframe_image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEIuxquuddI2",
        "outputId": "83244160-b179-4ea1-ad24-a113f142ffd8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dataframe_image\n",
            "  Downloading dataframe_image-0.1.11-py3-none-any.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (1.5.3)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (6.5.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (3.8.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (2.31.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (9.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (23.1)\n",
            "Requirement already satisfied: mistune in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (0.8.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (4.11.2)\n",
            "Collecting cssutils (from dataframe_image)\n",
            "  Downloading cssutils-2.7.1-py3-none-any.whl (399 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting html2image (from dataframe_image)\n",
            "  Downloading html2image-2.0.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (3.1.2)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (5.3.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (2.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (0.8.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (5.9.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (1.5.0)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (2.16.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (1.2.1)\n",
            "Requirement already satisfied: traitlets>=5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (5.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->dataframe_image) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->dataframe_image) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->dataframe_image) (1.23.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->dataframe_image) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->dataframe_image) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->dataframe_image) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->dataframe_image) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->dataframe_image) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->dataframe_image) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->dataframe_image) (1.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->dataframe_image) (2.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dataframe_image) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dataframe_image) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dataframe_image) (2023.7.22)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert>=5->dataframe_image) (3.10.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.5.0->nbconvert>=5->dataframe_image) (6.1.12)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert>=5->dataframe_image) (2.18.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert>=5->dataframe_image) (4.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.24->dataframe_image) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->dataframe_image) (0.5.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert>=5->dataframe_image) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert>=5->dataframe_image) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert>=5->dataframe_image) (0.9.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert>=5->dataframe_image) (23.2.1)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert>=5->dataframe_image) (6.3.1)\n",
            "Installing collected packages: html2image, cssutils, dataframe_image\n",
            "Successfully installed cssutils-2.7.1 dataframe_image-0.1.11 html2image-2.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dataframe_image as dfi"
      ],
      "metadata": {
        "id": "LV07chqJdgem"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO2axd4ldBmq"
      },
      "source": [
        "## Import all functions from another notebook for building prediction method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "34kIUwC3dBmr"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "#Parameters Settings\n",
        "\n",
        "max_features=7 # the feature number of each node is set to 8, as it is suggested to be one-third of the feature's number (Lahouar and Slama,2017)\n",
        "lr=0.005 #learning rate LSTM as recommended in (Kingma and Ba, 2014)\n",
        "optimizer='Adam' #Adam is used as the optimizer for LSTM as it is computationally efficient (Dubey, Ashutosh Kumar, et al,2021)\n",
        "neuron=32\n",
        "epoch=120 #(Jorges et al, 2021)\n",
        "batch_size=64 #(Kandel et al,2020)\n",
        "hours=24 #in this study, to predict one hour ahead predicton, we use input from the previous 24 hour energy consumption\n",
        "data_partition=0.8 #in this study, we divided the data into 80% of data as training data and 20% of the data as testing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk59gcd1dBmr"
      },
      "source": [
        "## Import parameter settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xKseC0pUdBmr"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from PyEMD import CEEMDAN\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "### import the libraries\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from math import sqrt\n",
        "\n",
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back, 0])\n",
        "    return numpy.array(dataX), numpy.array(dataY)\n",
        "\n",
        "def percentage_error(actual, predicted):\n",
        "    res = numpy.empty(actual.shape)\n",
        "    for j in range(actual.shape[0]):\n",
        "        if actual[j] != 0:\n",
        "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
        "        else:\n",
        "            res[j] = predicted[j] / np.mean(actual)\n",
        "    return res\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return numpy.mean(numpy.abs(percentage_error(numpy.asarray(y_true), numpy.asarray(y_pred)))) * 100\n",
        "\n",
        "\n",
        "\n",
        "# In[25]:\n",
        "\n",
        "\n",
        "def lr_model(datass,look_back,data_partition):\n",
        "\n",
        "    datasets=datass.values\n",
        "    train_size = int(len(datasets) * data_partition)\n",
        "    test_size = len(datasets) - train_size\n",
        "    train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "\n",
        "    trainX, trainY = create_dataset(train, look_back)\n",
        "    testX, testY = create_dataset(test, look_back)\n",
        "    X_train=pd.DataFrame(trainX)\n",
        "    Y_train=pd.DataFrame(trainY)\n",
        "    X_test=pd.DataFrame(testX)\n",
        "    Y_test=pd.DataFrame(testY)\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "\n",
        "    X= sc_X.fit_transform(X_train)\n",
        "    y= sc_y.fit_transform(Y_train)\n",
        "    X1= sc_X.fit_transform(X_test)\n",
        "    y1= sc_y.fit_transform(Y_test)\n",
        "    y=y.ravel()\n",
        "    y1=y1.ravel()\n",
        "    import tensorflow as tf\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "    tf.random.set_seed(1234)\n",
        "\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "\n",
        "    grid = LinearRegression()\n",
        "\n",
        "    grid.fit(X,y)\n",
        "    y_pred_train_lr= grid.predict(X)\n",
        "    y_pred_test_lr= grid.predict(X1)\n",
        "\n",
        "    y_pred_train_lr=pd.DataFrame(y_pred_train_lr)\n",
        "    y_pred_test_lr=pd.DataFrame(y_pred_test_lr)\n",
        "\n",
        "    y1=pd.DataFrame(y1)\n",
        "    y=pd.DataFrame(y)\n",
        "\n",
        "\n",
        "    y_pred_test1_lr= sc_y.inverse_transform (y_pred_test_lr)\n",
        "    y_pred_train1_lr=sc_y.inverse_transform (y_pred_train_lr)\n",
        "\n",
        "    y_test= sc_y.inverse_transform (y1)\n",
        "    y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "    y_pred_test1_rf=pd.DataFrame(y_pred_test1_lr)\n",
        "    y_pred_train1_rf=pd.DataFrame(y_pred_train1_lr)\n",
        "\n",
        "    y_test= pd.DataFrame(y_test)\n",
        "\n",
        "\n",
        "    #summarize the fit of the model\n",
        "    mape=mean_absolute_percentage_error(y_test,y_pred_test1_lr)\n",
        "    rmse= sqrt(mean_squared_error(y_test,y_pred_test1_lr))\n",
        "    mae=metrics.mean_absolute_error(y_test,y_pred_test1_lr)\n",
        "\n",
        "\n",
        "    return mape,rmse,mae\n",
        "\n",
        "\n",
        "# In[26]:\n",
        "\n",
        "\n",
        "def svr_model(datass,look_back,data_partition):\n",
        "\n",
        "    datasets=datass.values\n",
        "    train_size = int(len(datasets) * data_partition)\n",
        "    test_size = len(datasets) - train_size\n",
        "    train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "\n",
        "    trainX, trainY = create_dataset(train, look_back)\n",
        "    testX, testY = create_dataset(test, look_back)\n",
        "    X_train=pd.DataFrame(trainX)\n",
        "    Y_train=pd.DataFrame(trainY)\n",
        "    X_test=pd.DataFrame(testX)\n",
        "    Y_test=pd.DataFrame(testY)\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "\n",
        "    X= sc_X.fit_transform(X_train)\n",
        "    y= sc_y.fit_transform(Y_train)\n",
        "    X1= sc_X.fit_transform(X_test)\n",
        "    y1= sc_y.fit_transform(Y_test)\n",
        "    y=y.ravel()\n",
        "    y1=y1.ravel()\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "\n",
        "    import tensorflow as tf\n",
        "    tf.random.set_seed(1234)\n",
        "\n",
        "\n",
        "    from sklearn.svm import SVR\n",
        "\n",
        "    grid = SVR()\n",
        "    grid.fit(X,y)\n",
        "    y_pred_train_svr= grid.predict(X)\n",
        "    y_pred_test_svr= grid.predict(X1)\n",
        "\n",
        "    y_pred_train_svr=pd.DataFrame(y_pred_train_svr)\n",
        "    y_pred_test_svr=pd.DataFrame(y_pred_test_svr)\n",
        "\n",
        "    y1=pd.DataFrame(y1)\n",
        "    y=pd.DataFrame(y)\n",
        "\n",
        "\n",
        "    y_pred_test1_svr= sc_y.inverse_transform (y_pred_test_svr)\n",
        "    y_pred_train1_svr=sc_y.inverse_transform (y_pred_train_svr)\n",
        "\n",
        "    y_test= sc_y.inverse_transform (y1)\n",
        "    y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "    y_pred_test1_svr=pd.DataFrame(y_pred_test1_svr)\n",
        "    y_pred_train1_svr=pd.DataFrame(y_pred_train1_svr)\n",
        "\n",
        "    y_test= pd.DataFrame(y_test)\n",
        "\n",
        "\n",
        "    #summarize the fit of the model\n",
        "    mape=mean_absolute_percentage_error(y_test,y_pred_test1_svr)\n",
        "    rmse= sqrt(mean_squared_error(y_test,y_pred_test1_svr))\n",
        "    mae=metrics.mean_absolute_error(y_test,y_pred_test1_svr)\n",
        "\n",
        "\n",
        "    return mape,rmse,mae\n",
        "\n",
        "\n",
        "# In[27]:\n",
        "\n",
        "\n",
        "\n",
        "def ann_model(datass,look_back,data_partition):\n",
        "\n",
        "\n",
        "    datasets=datass.values\n",
        "    train_size = int(len(datasets) * data_partition)\n",
        "    test_size = len(datasets) - train_size\n",
        "    train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "    trainX, trainY = create_dataset(train, look_back)\n",
        "    testX, testY = create_dataset(test, look_back)\n",
        "    X_train=pd.DataFrame(trainX)\n",
        "    Y_train=pd.DataFrame(trainY)\n",
        "    X_test=pd.DataFrame(testX)\n",
        "    Y_test=pd.DataFrame(testY)\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "\n",
        "    X= sc_X.fit_transform(X_train)\n",
        "    y= sc_y.fit_transform(Y_train)\n",
        "    X1= sc_X.fit_transform(X_test)\n",
        "    y1= sc_y.fit_transform(Y_test)\n",
        "    y=y.ravel()\n",
        "    y1=y1.ravel()\n",
        "\n",
        "    import numpy\n",
        "\n",
        "    trainX = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
        "    testX = numpy.reshape(X1, (X1.shape[0], 1, X1.shape[1]))\n",
        "\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "    import tensorflow as tf\n",
        "    tf.random.set_seed(1234)\n",
        "\n",
        "\n",
        "    from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "    model= MLPRegressor(random_state=1,activation='tanh').fit(X,y)\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "\n",
        "\n",
        "    # make predictions\n",
        "    y_pred_train = model.predict(X)\n",
        "    y_pred_test = model.predict(X1)\n",
        "    y_pred_test= numpy.array(y_pred_test).ravel()\n",
        "\n",
        "    y_pred_test=pd.DataFrame(y_pred_test)\n",
        "    y1=pd.DataFrame(y1)\n",
        "\n",
        "    y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
        "    y_test= sc_y.inverse_transform (y1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #summarize the fit of the model\n",
        "    mape=mean_absolute_percentage_error(y_test,y_pred_test1)\n",
        "    rmse= sqrt(mean_squared_error(y_test,y_pred_test1))\n",
        "    mae=metrics.mean_absolute_error(y_test,y_pred_test1)\n",
        "\n",
        "\n",
        "    return mape,rmse,mae\n",
        "\n",
        "\n",
        "# In[28]:\n",
        "\n",
        "\n",
        "def rf_model(datass,look_back,data_partition,max_features):\n",
        "\n",
        "    datasets=datass.values\n",
        "    train_size = int(len(datasets) * data_partition)\n",
        "    test_size = len(datasets) - train_size\n",
        "    train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "\n",
        "    trainX, trainY = create_dataset(train, look_back)\n",
        "    testX, testY = create_dataset(test, look_back)\n",
        "    X_train=pd.DataFrame(trainX)\n",
        "    Y_train=pd.DataFrame(trainY)\n",
        "    X_test=pd.DataFrame(testX)\n",
        "    Y_test=pd.DataFrame(testY)\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "\n",
        "    X= sc_X.fit_transform(X_train)\n",
        "    y= sc_y.fit_transform(Y_train)\n",
        "    X1= sc_X.fit_transform(X_test)\n",
        "    y1= sc_y.fit_transform(Y_test)\n",
        "    y=y.ravel()\n",
        "    y1=y1.ravel()\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "    import tensorflow as tf\n",
        "    tf.random.set_seed(1234)\n",
        "\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "    grid = RandomForestRegressor(max_features=max_features)\n",
        "    grid.fit(X,y)\n",
        "    y_pred_train_rf= grid.predict(X)\n",
        "    y_pred_test_rf= grid.predict(X1)\n",
        "\n",
        "    y_pred_train_rf=pd.DataFrame(y_pred_train_rf)\n",
        "    y_pred_test_rf=pd.DataFrame(y_pred_test_rf)\n",
        "\n",
        "    y1=pd.DataFrame(y1)\n",
        "    y=pd.DataFrame(y)\n",
        "\n",
        "\n",
        "    y_pred_test1_rf= sc_y.inverse_transform (y_pred_test_rf)\n",
        "    y_pred_train1_rf=sc_y.inverse_transform (y_pred_train_rf)\n",
        "\n",
        "    y_test= sc_y.inverse_transform (y1)\n",
        "    y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "    y_pred_test1_rf=pd.DataFrame(y_pred_test1_rf)\n",
        "    y_pred_train1_rf=pd.DataFrame(y_pred_train1_rf)\n",
        "\n",
        "    y_test= pd.DataFrame(y_test)\n",
        "\n",
        "\n",
        "    #summarize the fit of the model\n",
        "    mape=mean_absolute_percentage_error(y_test,y_pred_test1_rf)\n",
        "    rmse= sqrt(mean_squared_error(y_test,y_pred_test1_rf))\n",
        "    mae=metrics.mean_absolute_error(y_test,y_pred_test1_rf)\n",
        "\n",
        "\n",
        "    return mape,rmse,mae\n",
        "\n",
        "\n",
        "# In[29]:\n",
        "\n",
        "\n",
        "def lstm_model(datass,look_back,data_partition,max_features,epoch,batch_size,neuron,lr,optimizer):\n",
        "    datasets=datass.values\n",
        "    train_size = int(len(datasets) * data_partition)\n",
        "    test_size = len(datasets) - train_size\n",
        "    train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "    trainX, trainY = create_dataset(train, look_back)\n",
        "    testX, testY = create_dataset(test, look_back)\n",
        "    X_train=pd.DataFrame(trainX)\n",
        "    Y_train=pd.DataFrame(trainY)\n",
        "    X_test=pd.DataFrame(testX)\n",
        "    Y_test=pd.DataFrame(testY)\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "\n",
        "    X= sc_X.fit_transform(X_train)\n",
        "    y= sc_y.fit_transform(Y_train)\n",
        "    X1= sc_X.fit_transform(X_test)\n",
        "    y1= sc_y.fit_transform(Y_test)\n",
        "    y=y.ravel()\n",
        "    y1=y1.ravel()\n",
        "    trainX1 = numpy.reshape(X, (X.shape[0],1,X.shape[1]))\n",
        "    testX1 = numpy.reshape(X1, (X1.shape[0],1,X1.shape[1]))\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "    import tensorflow as tf\n",
        "    tf.random.set_seed(1234)\n",
        "\n",
        "    import os\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers.core import Dense, Dropout, Activation\n",
        "    from keras.layers import LSTM\n",
        "\n",
        "\n",
        "    neuron=neuron\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units = neuron,input_shape=(trainX1.shape[1], trainX1.shape[2])))\n",
        "    model.add(Dense(1))\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(loss='mse',optimizer=optimizer)\n",
        "#    model.summary()\n",
        "\n",
        "\n",
        "  # Fitting the RNN to the Training s\n",
        "    model.fit(trainX1, y, epochs = epoch, batch_size = batch_size,verbose=0)\n",
        "  # make predictions\n",
        "    y_pred_train = model.predict(trainX1)\n",
        "    y_pred_test = model.predict(testX1)\n",
        "    y_pred_test= numpy.array(y_pred_test).ravel()\n",
        "\n",
        "    y_pred_test=pd.DataFrame(y_pred_test)\n",
        "    y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
        "    y1=pd.DataFrame(y1)\n",
        "\n",
        "    y_test= sc_y.inverse_transform (y1)\n",
        "\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "    from sklearn import metrics\n",
        "\n",
        "    mape=mean_absolute_percentage_error(y_test,y_pred_test1)\n",
        "    rmse= sqrt(mean_squared_error(y_test,y_pred_test1))\n",
        "    mae=metrics.mean_absolute_error(y_test,y_pred_test1)\n",
        "    return mape,rmse,mae\n",
        "\n",
        "\n",
        "# In[30]:\n",
        "\n",
        "\n",
        "###################################################hybrid based ceemdan####################################################\n",
        "def hybrid_ceemdan_rf(datass,look_back,data_partition,max_features):\n",
        "\n",
        "\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "\n",
        "    dfs=datass\n",
        "    s = dfs.values\n",
        "\n",
        "    emd = CEEMDAN(epsilon=0.08)\n",
        "    emd.noise_seed(12345)\n",
        "\n",
        "    IMFs = emd(s)\n",
        "\n",
        "\n",
        "    full_imf=pd.DataFrame(IMFs)\n",
        "    data_imf=full_imf.T\n",
        "\n",
        "    import pandas as pd\n",
        "\n",
        "    pred_test=[]\n",
        "    test_ori=[]\n",
        "    pred_train=[]\n",
        "    train_ori=[]\n",
        "\n",
        "\n",
        "    for col in data_imf:\n",
        "\n",
        "        datasetss2=pd.DataFrame(data_imf[col])\n",
        "        datasets=datasetss2.values\n",
        "        train_size = int(len(datasets) * data_partition)\n",
        "        test_size = len(datasets) - train_size\n",
        "        train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "\n",
        "        trainX, trainY = create_dataset(train, look_back)\n",
        "        testX, testY = create_dataset(test, look_back)\n",
        "        X_train=pd.DataFrame(trainX)\n",
        "        Y_train=pd.DataFrame(trainY)\n",
        "        X_test=pd.DataFrame(testX)\n",
        "        Y_test=pd.DataFrame(testY)\n",
        "        sc_X = StandardScaler()\n",
        "        sc_y = StandardScaler()\n",
        "        X= sc_X.fit_transform(X_train)\n",
        "        y= sc_y.fit_transform(Y_train)\n",
        "        X1= sc_X.fit_transform(X_test)\n",
        "        y1= sc_y.fit_transform(Y_test)\n",
        "        y=y.ravel()\n",
        "        y1=y1.ravel()\n",
        "\n",
        "        import numpy\n",
        "\n",
        "        numpy.random.seed(1234)\n",
        "        import tensorflow as tf\n",
        "        tf.random.set_seed(1234)\n",
        "\n",
        "\n",
        "        from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "        grid = RandomForestRegressor(max_features=max_features)\n",
        "        grid.fit(X,y)\n",
        "        y_pred_train= grid.predict(X)\n",
        "        y_pred_test= grid.predict(X1)\n",
        "\n",
        "        y_pred_test=pd.DataFrame(y_pred_test)\n",
        "        y_pred_train=pd.DataFrame(y_pred_train)\n",
        "\n",
        "        y1=pd.DataFrame(y1)\n",
        "        y=pd.DataFrame(y)\n",
        "\n",
        "        y_test= sc_y.inverse_transform (y1)\n",
        "        y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "        y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
        "        y_pred_train1= sc_y.inverse_transform (y_pred_train)\n",
        "\n",
        "\n",
        "        pred_test.append(y_pred_test1)\n",
        "        test_ori.append(y_test)\n",
        "        pred_train.append(y_pred_train1)\n",
        "        train_ori.append(y_train)\n",
        "\n",
        "\n",
        "    result_pred_test= pd.DataFrame.from_records(pred_test)\n",
        "    result_pred_train= pd.DataFrame.from_records(pred_train)\n",
        "\n",
        "\n",
        "    a=result_pred_test.sum(axis = 0, skipna = True)\n",
        "    b=result_pred_train.sum(axis = 0, skipna = True)\n",
        "\n",
        "\n",
        "    dataframe=pd.DataFrame(dfs)\n",
        "    dataset=dataframe.values\n",
        "\n",
        "    train_size = int(len(dataset) * data_partition)\n",
        "    test_size = len(dataset) - train_size\n",
        "    train, test = dataset[0:train_size], dataset[train_size:len(dataset)]\n",
        "\n",
        "    trainX, trainY = create_dataset(train, look_back)\n",
        "    testX, testY = create_dataset(test, look_back)\n",
        "    X_train=pd.DataFrame(trainX)\n",
        "    Y_train=pd.DataFrame(trainY)\n",
        "    X_test=pd.DataFrame(testX)\n",
        "    Y_test=pd.DataFrame(testY)\n",
        "\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "    X= sc_X.fit_transform(X_train)\n",
        "    y= sc_y.fit_transform(Y_train)\n",
        "    X1= sc_X.fit_transform(X_test)\n",
        "    y1= sc_y.fit_transform(Y_test)\n",
        "    y=y.ravel()\n",
        "    y1=y1.ravel()\n",
        "\n",
        "\n",
        "    trainX = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
        "    testX = numpy.reshape(X1, (X1.shape[0], 1, X1.shape[1]))\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "    import tensorflow as tf\n",
        "    tf.random.set_seed(1234)\n",
        "\n",
        "    y1=pd.DataFrame(y1)\n",
        "    y=pd.DataFrame(y)\n",
        "    y_test= sc_y.inverse_transform (y1)\n",
        "    y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    a= pd.DataFrame(a)\n",
        "    y_test= pd.DataFrame(y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   #summarize the fit of the model\n",
        "    mape=mean_absolute_percentage_error(y_test,a)\n",
        "    rmse= sqrt(mean_squared_error(y_test,a))\n",
        "    mae=metrics.mean_absolute_error(y_test,a)\n",
        "\n",
        "\n",
        "\n",
        "    return mape,rmse,mae\n",
        "\n",
        "\n",
        "# In[31]:\n",
        "\n",
        "\n",
        "def hybrid_ceemdan_lstm(datass,look_back,data_partition,max_features,epoch,batch_size,neuron,lr,optimizer):\n",
        "\n",
        "\n",
        "    from PyEMD import CEEMDAN\n",
        "\n",
        "    dfs=datass\n",
        "    s = dfs.values\n",
        "\n",
        "    emd = CEEMDAN(epsilon=0.08)\n",
        "    emd.noise_seed(12345)\n",
        "\n",
        "    IMFs = emd(s)\n",
        "\n",
        "\n",
        "    full_imf=pd.DataFrame(IMFs)\n",
        "    data_imf=full_imf.T\n",
        "\n",
        "    pred_test=[]\n",
        "    test_ori=[]\n",
        "    pred_train=[]\n",
        "    train_ori=[]\n",
        "\n",
        "\n",
        "    for col in data_imf:\n",
        "\n",
        "        datasetss2=pd.DataFrame(data_imf[col])\n",
        "        datasets=datasetss2.values\n",
        "        train_size = int(len(datasets) * data_partition)\n",
        "        test_size = len(datasets) - train_size\n",
        "        train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "\n",
        "        trainX, trainY = create_dataset(train, look_back)\n",
        "        testX, testY = create_dataset(test, look_back)\n",
        "        X_train=pd.DataFrame(trainX)\n",
        "        Y_train=pd.DataFrame(trainY)\n",
        "        X_test=pd.DataFrame(testX)\n",
        "        Y_test=pd.DataFrame(testY)\n",
        "        sc_X = StandardScaler()\n",
        "        sc_y = StandardScaler()\n",
        "        X= sc_X.fit_transform(X_train)\n",
        "        y= sc_y.fit_transform(Y_train)\n",
        "        X1= sc_X.fit_transform(X_test)\n",
        "        y1= sc_y.fit_transform(Y_test)\n",
        "        y=y.ravel()\n",
        "        y1=y1.ravel()\n",
        "\n",
        "        import numpy\n",
        "\n",
        "        trainX = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
        "        testX = numpy.reshape(X1, (X1.shape[0], 1, X1.shape[1]))\n",
        "\n",
        "        numpy.random.seed(1234)\n",
        "        import tensorflow as tf\n",
        "        tf.random.set_seed(1234)\n",
        "\n",
        "\n",
        "        from keras.models import Sequential\n",
        "        from keras.layers.core import Dense, Dropout, Activation\n",
        "        from keras.layers import LSTM\n",
        "\n",
        "\n",
        "        neuron=neuron\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(units = neuron,input_shape=(trainX.shape[1], trainX.shape[2])))\n",
        "        model.add(Dense(1))\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        model.compile(loss='mse',optimizer=optimizer)\n",
        "\n",
        "        numpy.random.seed(1234)\n",
        "\n",
        "\n",
        "        # Fitting the RNN to the Training set\n",
        "        model.fit(trainX, y, epochs = epoch, batch_size = batch_size,verbose=0)\n",
        "\n",
        "        # make predictions\n",
        "        y_pred_train = model.predict(trainX)\n",
        "        y_pred_test = model.predict(testX)\n",
        "\n",
        "        # make predictions\n",
        "\n",
        "        y_pred_test= numpy.array(y_pred_test).ravel()\n",
        "        y_pred_test=pd.DataFrame(y_pred_test)\n",
        "        y1=pd.DataFrame(y1)\n",
        "        y=pd.DataFrame(y)\n",
        "        y_pred_train= numpy.array(y_pred_train).ravel()\n",
        "        y_pred_train=pd.DataFrame(y_pred_train)\n",
        "\n",
        "\n",
        "        y_test= sc_y.inverse_transform (y1)\n",
        "        y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "        y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
        "        y_pred_train1= sc_y.inverse_transform (y_pred_train)\n",
        "\n",
        "\n",
        "        pred_test.append(y_pred_test1)\n",
        "        test_ori.append(y_test)\n",
        "        pred_train.append(y_pred_train1)\n",
        "        train_ori.append(y_train)\n",
        "\n",
        "\n",
        "    result_pred_test= pd.DataFrame.from_records(pred_test)\n",
        "    result_pred_train= pd.DataFrame.from_records(pred_train)\n",
        "\n",
        "\n",
        "    a=result_pred_test.sum(axis = 0, skipna = True)\n",
        "    b=result_pred_train.sum(axis = 0, skipna = True)\n",
        "\n",
        "\n",
        "    dataframe=pd.DataFrame(dfs)\n",
        "    dataset=dataframe.values\n",
        "\n",
        "    train_size = int(len(dataset) * data_partition)\n",
        "    test_size = len(dataset) - train_size\n",
        "    train, test = dataset[0:train_size], dataset[train_size:len(dataset)]\n",
        "\n",
        "    trainX, trainY = create_dataset(train, look_back)\n",
        "    testX, testY = create_dataset(test, look_back)\n",
        "    X_train=pd.DataFrame(trainX)\n",
        "    Y_train=pd.DataFrame(trainY)\n",
        "    X_test=pd.DataFrame(testX)\n",
        "    Y_test=pd.DataFrame(testY)\n",
        "\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "    X= sc_X.fit_transform(X_train)\n",
        "    y= sc_y.fit_transform(Y_train)\n",
        "    X1= sc_X.fit_transform(X_test)\n",
        "    y1= sc_y.fit_transform(Y_test)\n",
        "    y=y.ravel()\n",
        "    y1=y1.ravel()\n",
        "\n",
        "\n",
        "    trainX = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
        "    testX = numpy.reshape(X1, (X1.shape[0], 1, X1.shape[1]))\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "    import tensorflow as tf\n",
        "    tf.random.set_seed(1234)\n",
        "\n",
        "    y1=pd.DataFrame(y1)\n",
        "    y=pd.DataFrame(y)\n",
        "\n",
        "    y_test= sc_y.inverse_transform (y1)\n",
        "    y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "\n",
        "    a= pd.DataFrame(a)\n",
        "    y_test= pd.DataFrame(y_test)\n",
        "\n",
        "\n",
        "\n",
        "   #summarize the fit of the model\n",
        "    mape=mean_absolute_percentage_error(y_test,a)\n",
        "    rmse= sqrt(mean_squared_error(y_test,a))\n",
        "    mae=metrics.mean_absolute_error(y_test,a)\n",
        "\n",
        "    return mape,rmse,mae\n",
        "\n",
        "\n",
        "# In[32]:\n",
        "\n",
        "\n",
        "def proposed_method(datass,look_back,data_partition,max_features,epoch,batch_size,neuron,lr,optimizer):\n",
        "\n",
        "    from PyEMD import CEEMDAN\n",
        "\n",
        "\n",
        "    dfs=datass\n",
        "    s = dfs.values\n",
        "\n",
        "    emd = CEEMDAN(epsilon=0.05)\n",
        "    emd.noise_seed(12345)\n",
        "\n",
        "    IMFs = emd(s)\n",
        "\n",
        "\n",
        "    full_imf=pd.DataFrame(IMFs)\n",
        "    data_imf=full_imf.T\n",
        "\n",
        "\n",
        "\n",
        "    pred_test=[]\n",
        "    test_ori=[]\n",
        "    pred_train=[]\n",
        "    train_ori=[]\n",
        "\n",
        "\n",
        "    n_imf=len(data_imf.columns)\n",
        "\n",
        "    k=list(range(1,n_imf))\n",
        "    m=[0]\n",
        "\n",
        "\n",
        "    for i in m:\n",
        "\n",
        "        datasetss2=pd.DataFrame(data_imf[i])\n",
        "        datasets=datasetss2.values\n",
        "        train_size = int(len(datasets) * data_partition)\n",
        "        test_size = len(datasets) - train_size\n",
        "        train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "\n",
        "        trainX, trainY = create_dataset(train, look_back)\n",
        "        testX, testY = create_dataset(test, look_back)\n",
        "        X_train=pd.DataFrame(trainX)\n",
        "        Y_train=pd.DataFrame(trainY)\n",
        "        X_test=pd.DataFrame(testX)\n",
        "        Y_test=pd.DataFrame(testY)\n",
        "        sc_X = StandardScaler()\n",
        "        sc_y = StandardScaler()\n",
        "        X= sc_X.fit_transform(X_train)\n",
        "        y= sc_y.fit_transform(Y_train)\n",
        "        X1= sc_X.fit_transform(X_test)\n",
        "        y1= sc_y.fit_transform(Y_test)\n",
        "        y=y.ravel()\n",
        "        y1=y1.ravel()\n",
        "\n",
        "        import numpy\n",
        "\n",
        "        trainX = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
        "        testX = numpy.reshape(X1, (X1.shape[0], 1, X1.shape[1]))\n",
        "\n",
        "        numpy.random.seed(1234)\n",
        "        import tensorflow as tf\n",
        "        tf.random.set_seed(1234)\n",
        "\n",
        "\n",
        "        from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "        grid = RandomForestRegressor(max_features=max_features)\n",
        "        grid.fit(X,y)\n",
        "        y_pred_train= grid.predict(X)\n",
        "        y_pred_test= grid.predict(X1)\n",
        "\n",
        "        y_pred_test=pd.DataFrame(y_pred_test)\n",
        "        y_pred_train=pd.DataFrame(y_pred_train)\n",
        "\n",
        "        y1=pd.DataFrame(y1)\n",
        "        y=pd.DataFrame(y)\n",
        "\n",
        "        y_test= sc_y.inverse_transform (y1)\n",
        "        y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "        y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
        "        y_pred_train1= sc_y.inverse_transform (y_pred_train)\n",
        "\n",
        "\n",
        "        pred_test.append(y_pred_test1)\n",
        "        test_ori.append(y_test)\n",
        "        pred_train.append(y_pred_train1)\n",
        "        train_ori.append(y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for i in k:\n",
        "\n",
        "        datasetss2=pd.DataFrame(data_imf[i])\n",
        "        datasets=datasetss2.values\n",
        "        train_size = int(len(datasets) * data_partition)\n",
        "        test_size = len(datasets) - train_size\n",
        "        train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "\n",
        "        trainX, trainY = create_dataset(train, look_back)\n",
        "        testX, testY = create_dataset(test, look_back)\n",
        "        X_train=pd.DataFrame(trainX)\n",
        "        Y_train=pd.DataFrame(trainY)\n",
        "        X_test=pd.DataFrame(testX)\n",
        "        Y_test=pd.DataFrame(testY)\n",
        "        sc_X = StandardScaler()\n",
        "        sc_y = StandardScaler()\n",
        "        X= sc_X.fit_transform(X_train)\n",
        "        y= sc_y.fit_transform(Y_train)\n",
        "        X1= sc_X.fit_transform(X_test)\n",
        "        y1= sc_y.fit_transform(Y_test)\n",
        "        y=y.ravel()\n",
        "        y1=y1.ravel()\n",
        "\n",
        "        import numpy\n",
        "\n",
        "        trainX = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
        "        testX = numpy.reshape(X1, (X1.shape[0], 1, X1.shape[1]))\n",
        "\n",
        "        numpy.random.seed(1234)\n",
        "        import tensorflow as tf\n",
        "        tf.random.set_seed(1234)\n",
        "\n",
        "\n",
        "        from keras.models import Sequential\n",
        "        from keras.layers.core import Dense, Dropout, Activation\n",
        "        from keras.layers import LSTM\n",
        "\n",
        "        neuron=neuron\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(units = neuron,input_shape=(trainX.shape[1], trainX.shape[2])))\n",
        "        model.add(Dense(1))\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        model.compile(loss='mse',optimizer=optimizer)\n",
        "\n",
        "\n",
        "        numpy.random.seed(1234)\n",
        "\n",
        "\n",
        "     # Fitting the RNN to the Training set\n",
        "        model.fit(trainX, y, epochs = epoch, batch_size = batch_size,verbose=0)\n",
        "\n",
        "\n",
        "        # make predictions\n",
        "        y_pred_train = model.predict(trainX)\n",
        "        y_pred_test = model.predict(testX)\n",
        "\n",
        "        # make predictions\n",
        "\n",
        "\n",
        "        y_pred_test= numpy.array(y_pred_test).ravel()\n",
        "        y_pred_test=pd.DataFrame(y_pred_test)\n",
        "        y1=pd.DataFrame(y1)\n",
        "        y=pd.DataFrame(y)\n",
        "        y_pred_train= numpy.array(y_pred_train).ravel()\n",
        "        y_pred_train=pd.DataFrame(y_pred_train)\n",
        "\n",
        "\n",
        "        y_test= sc_y.inverse_transform (y1)\n",
        "        y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "        y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
        "        y_pred_train1= sc_y.inverse_transform (y_pred_train)\n",
        "\n",
        "\n",
        "        pred_test.append(y_pred_test1)\n",
        "        test_ori.append(y_test)\n",
        "        pred_train.append(y_pred_train1)\n",
        "        train_ori.append(y_train)\n",
        "\n",
        "    result_pred_test= pd.DataFrame.from_records(pred_test)\n",
        "    result_pred_train= pd.DataFrame.from_records(pred_train)\n",
        "\n",
        "\n",
        "    a=result_pred_test.sum(axis = 0, skipna = True)\n",
        "    b=result_pred_train.sum(axis = 0, skipna = True)\n",
        "\n",
        "\n",
        "    dataframe=pd.DataFrame(dfs)\n",
        "    dataset=dataframe.values\n",
        "\n",
        "    train_size = int(len(dataset) * data_partition)\n",
        "    test_size = len(dataset) - train_size\n",
        "    train, test = dataset[0:train_size], dataset[train_size:len(dataset)]\n",
        "\n",
        "    trainX, trainY = create_dataset(train, look_back)\n",
        "    testX, testY = create_dataset(test, look_back)\n",
        "    X_train=pd.DataFrame(trainX)\n",
        "    Y_train=pd.DataFrame(trainY)\n",
        "    X_test=pd.DataFrame(testX)\n",
        "    Y_test=pd.DataFrame(testY)\n",
        "\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "    X= sc_X.fit_transform(X_train)\n",
        "    y= sc_y.fit_transform(Y_train)\n",
        "    X1= sc_X.fit_transform(X_test)\n",
        "    y1= sc_y.fit_transform(Y_test)\n",
        "    y=y.ravel()\n",
        "    y1=y1.ravel()\n",
        "\n",
        "\n",
        "    trainX = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
        "    testX = numpy.reshape(X1, (X1.shape[0], 1, X1.shape[1]))\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "    import tensorflow as tf\n",
        "    tf.random.set_seed(1234)\n",
        "\n",
        "    y1=pd.DataFrame(y1)\n",
        "    y=pd.DataFrame(y)\n",
        "    y_test= sc_y.inverse_transform (y1)\n",
        "    y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "\n",
        "    a= pd.DataFrame(a)\n",
        "    y_test= pd.DataFrame(y_test)\n",
        "\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "\n",
        "   #summarize the fit of the model\n",
        "    mape=mean_absolute_percentage_error(y_test,a)\n",
        "    rmse= sqrt(mean_squared_error(y_test,a))\n",
        "    mae=metrics.mean_absolute_error(y_test,a)\n",
        "\n",
        "    return mape,rmse,mae,a,y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8EASnNkdBmr"
      },
      "source": [
        "## Run the experiments\n",
        "### Run this following cell will train and test the proposed method and other benchmark methods on Office Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISnjFlKCdBmr",
        "outputId": "27a57493-8d07-4289-ed16-c078e3bd439a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 0.22778034210205078 seconds - Linear Regression- office ---\n",
            "--- 0.6755959987640381 seconds - Support Vector Regression- office ---\n",
            "--- 4.533452033996582 seconds - ANN- office ---\n",
            "--- 1.6200287342071533 seconds - Random Forest- office ---\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "--- 11.129384994506836 seconds - lstm- office ---\n",
            "--- 34.79765439033508 seconds - ceemdan_rf- office ---\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 1ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 1s 1ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 1ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "--- 133.0010015964508 seconds - ceemdan_lstm- office ---\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 3ms/step\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 1ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 1ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "--- 129.14832997322083 seconds - proposed_method- office ---\n"
          ]
        }
      ],
      "source": [
        "#Linear Regression\n",
        "\n",
        "start_time = time.time()\n",
        "lr_office=lr_model(datas_office,hours,data_partition)\n",
        "lr_time_office=time.time() - start_time\n",
        "print(\"--- %s seconds - Linear Regression- office ---\" % (lr_time_office))\n",
        "\n",
        "#Support Vector Regression\n",
        "start_time = time.time()\n",
        "svr_office=svr_model(datas_office,hours,data_partition)\n",
        "svr_time_office=time.time() - start_time\n",
        "print(\"--- %s seconds - Support Vector Regression- office ---\" % (svr_time_office))\n",
        "\n",
        "\n",
        "#ANN\n",
        "start_time = time.time()\n",
        "ann_office=ann_model(datas_office,hours,data_partition)\n",
        "ann_time_office=time.time() - start_time\n",
        "print(\"--- %s seconds - ANN- office ---\" % (ann_time_office))\n",
        "\n",
        "#random forest\n",
        "start_time = time.time()\n",
        "rf_office=rf_model(datas_office,hours,data_partition,max_features)\n",
        "rf_time_office=time.time() - start_time\n",
        "print(\"--- %s seconds - Random Forest- office ---\" % (rf_time_office))\n",
        "\n",
        "#LSTM\n",
        "start_time = time.time()\n",
        "lstm_office=lstm_model(datas_office,hours,data_partition,max_features,epoch,batch_size,neuron,lr,optimizer)\n",
        "lstm_time_office=time.time() - start_time\n",
        "print(\"--- %s seconds - lstm- office ---\" % (lstm_time_office))\n",
        "\n",
        "\n",
        "#CEEMDAN RF\n",
        "start_time = time.time()\n",
        "ceemdan_rf_office=hybrid_ceemdan_rf(dfs_office,hours,data_partition,max_features)\n",
        "ceemdan_rf_time_office=time.time() - start_time\n",
        "print(\"--- %s seconds - ceemdan_rf- office ---\" % (ceemdan_rf_time_office))\n",
        "\n",
        "#CEEMDAN LSTM\n",
        "start_time = time.time()\n",
        "ceemdan_lstm_office=hybrid_ceemdan_lstm(dfs_office,hours,data_partition,max_features,epoch,batch_size,neuron,lr,optimizer)\n",
        "ceemdan_lstm_time_office=time.time() - start_time\n",
        "print(\"--- %s seconds - ceemdan_lstm- office ---\" % (ceemdan_lstm_time_office))\n",
        "\n",
        "\n",
        "#proposed method\n",
        "start_time = time.time()\n",
        "proposed_method_office=proposed_method(dfs_office,hours,data_partition,max_features,epoch,batch_size,neuron,lr,optimizer)\n",
        "proposed_method_time_office=time.time() - start_time\n",
        "print(\"--- %s seconds - proposed_method- office ---\" % (proposed_method_time_office))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvdswwcWdBms"
      },
      "source": [
        "## Summarize of experimental results with running time\n",
        "### Run this following cell will summarize the result and generate output used in Section 4.4 (Table 3) for Office dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dFq0MXAKdBms",
        "outputId": "0f95b6af-2739-4599-969e-d682028342ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       LR       SVR        ANN         RF       LSTM  \\\n",
              "office results                                                         \n",
              "0               11.169459  9.306992  11.291101  10.191593   9.871718   \n",
              "1                1.217669  1.106114   1.148319   1.096849   1.125255   \n",
              "2                0.901621  0.769583   0.876292   0.807782   0.810891   \n",
              "0                0.227780  0.675596   4.533452   1.620029  11.129385   \n",
              "\n",
              "                CEEMDAN RF  CEEMDAN LSTM  Proposed Method  \n",
              "office results                                             \n",
              "0                 6.209099      6.406059         5.354160  \n",
              "1                 0.654080      0.669977         0.570419  \n",
              "2                 0.494386      0.511037         0.432533  \n",
              "0                34.797654    133.001002       129.148330  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-5ce42aa1-53f1-4a4e-a3d4-760939017557\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LR</th>\n",
              "      <th>SVR</th>\n",
              "      <th>ANN</th>\n",
              "      <th>RF</th>\n",
              "      <th>LSTM</th>\n",
              "      <th>CEEMDAN RF</th>\n",
              "      <th>CEEMDAN LSTM</th>\n",
              "      <th>Proposed Method</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>office results</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11.169459</td>\n",
              "      <td>9.306992</td>\n",
              "      <td>11.291101</td>\n",
              "      <td>10.191593</td>\n",
              "      <td>9.871718</td>\n",
              "      <td>6.209099</td>\n",
              "      <td>6.406059</td>\n",
              "      <td>5.354160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.217669</td>\n",
              "      <td>1.106114</td>\n",
              "      <td>1.148319</td>\n",
              "      <td>1.096849</td>\n",
              "      <td>1.125255</td>\n",
              "      <td>0.654080</td>\n",
              "      <td>0.669977</td>\n",
              "      <td>0.570419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.901621</td>\n",
              "      <td>0.769583</td>\n",
              "      <td>0.876292</td>\n",
              "      <td>0.807782</td>\n",
              "      <td>0.810891</td>\n",
              "      <td>0.494386</td>\n",
              "      <td>0.511037</td>\n",
              "      <td>0.432533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.227780</td>\n",
              "      <td>0.675596</td>\n",
              "      <td>4.533452</td>\n",
              "      <td>1.620029</td>\n",
              "      <td>11.129385</td>\n",
              "      <td>34.797654</td>\n",
              "      <td>133.001002</td>\n",
              "      <td>129.148330</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ce42aa1-53f1-4a4e-a3d4-760939017557')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-d82943d9-2f1c-476d-a593-2eb71ee659c0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d82943d9-2f1c-476d-a593-2eb71ee659c0')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-d82943d9-2f1c-476d-a593-2eb71ee659c0 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5ce42aa1-53f1-4a4e-a3d4-760939017557 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5ce42aa1-53f1-4a4e-a3d4-760939017557');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "running_time_office=pd.DataFrame([lr_time_office,svr_time_office,ann_time_office,\n",
        "                                   rf_time_office,lstm_time_office,ceemdan_rf_time_office,\n",
        "                                   ceemdan_lstm_time_office,proposed_method_time_office])\n",
        "running_time_office=running_time_office.T\n",
        "running_time_office.columns=['LR','SVR','ANN','RF','LSTM','CEEMDAN RF','CEEMDAN LSTM','Proposed Method']\n",
        "\n",
        "\n",
        "proposed_method_office_df=proposed_method_office[0:3]\n",
        "result_office=pd.DataFrame([lr_office,svr_office,ann_office,rf_office,lstm_office,ceemdan_rf_office,\n",
        "                    ceemdan_lstm_office,proposed_method_office_df])\n",
        "result_office=result_office.T\n",
        "result_office.columns=['LR','SVR','ANN','RF','LSTM','CEEMDAN RF','CEEMDAN LSTM','Proposed Method']\n",
        "office_summary=pd.concat([result_office,running_time_office],axis=0)\n",
        "\n",
        "office_summary.set_axis(['MAPE(%)', 'RMSE','MAE','running time (s)'], axis='index')\n",
        "\n",
        "office_summary.style.set_caption(\"Office Results\")\n",
        "index = office_summary.index\n",
        "index.name = \"office results\"\n",
        "office_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-jIWd-YdBmt"
      },
      "outputs": [],
      "source": [
        "#export table to png\n",
        "#dfi.export(office_summary,\"office_summary_table.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHzV9IsxdBmt"
      },
      "source": [
        "## Calculate percentage improvement\n",
        "### Run this following cell will calculate percentage improvement and generate output used in Section 4.4 (Table 4) for Office dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "z1dq0-nydBmt",
        "outputId": "12ed4632-1c8f-4b89-e528-ac0124daa133"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               Proposed Method vs.LR  Proposed Method vs.SVR  \\\n",
              "Percentage Improvement office                                                  \n",
              "0                                              52.06                   42.47   \n",
              "1                                              53.15                   48.43   \n",
              "2                                              52.03                   43.80   \n",
              "\n",
              "                                Proposed Method vs.ANN  Proposed Method vs.RF  \\\n",
              "Percentage Improvement office                                                   \n",
              "0                                                52.58                  47.46   \n",
              "1                                                50.33                  47.99   \n",
              "2                                                50.64                  46.45   \n",
              "\n",
              "                               Proposed Method vs.LSTM  \\\n",
              "Percentage Improvement office                            \n",
              "0                                                45.76   \n",
              "1                                                49.31   \n",
              "2                                                46.66   \n",
              "\n",
              "                               Proposed Method vs.CEEMDAN RF  \\\n",
              "Percentage Improvement office                                  \n",
              "0                                                      13.77   \n",
              "1                                                      12.79   \n",
              "2                                                      12.51   \n",
              "\n",
              "                               Proposed Method vs. CEEMDAN LSTM  \n",
              "Percentage Improvement office                                    \n",
              "0                                                         16.42  \n",
              "1                                                         14.86  \n",
              "2                                                         15.36  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-ea6cb441-71a5-41a2-b20d-8d81b54490b5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Proposed Method vs.LR</th>\n",
              "      <th>Proposed Method vs.SVR</th>\n",
              "      <th>Proposed Method vs.ANN</th>\n",
              "      <th>Proposed Method vs.RF</th>\n",
              "      <th>Proposed Method vs.LSTM</th>\n",
              "      <th>Proposed Method vs.CEEMDAN RF</th>\n",
              "      <th>Proposed Method vs. CEEMDAN LSTM</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Percentage Improvement office</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>52.06</td>\n",
              "      <td>42.47</td>\n",
              "      <td>52.58</td>\n",
              "      <td>47.46</td>\n",
              "      <td>45.76</td>\n",
              "      <td>13.77</td>\n",
              "      <td>16.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>53.15</td>\n",
              "      <td>48.43</td>\n",
              "      <td>50.33</td>\n",
              "      <td>47.99</td>\n",
              "      <td>49.31</td>\n",
              "      <td>12.79</td>\n",
              "      <td>14.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>52.03</td>\n",
              "      <td>43.80</td>\n",
              "      <td>50.64</td>\n",
              "      <td>46.45</td>\n",
              "      <td>46.66</td>\n",
              "      <td>12.51</td>\n",
              "      <td>15.36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea6cb441-71a5-41a2-b20d-8d81b54490b5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-7dc64bdb-fc7e-4523-8e16-937d65ba7911\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7dc64bdb-fc7e-4523-8e16-937d65ba7911')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-7dc64bdb-fc7e-4523-8e16-937d65ba7911 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ea6cb441-71a5-41a2-b20d-8d81b54490b5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ea6cb441-71a5-41a2-b20d-8d81b54490b5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "pMAPE_LR_vs_Proposed_office=((lr_office[0]-proposed_method_office[0])/lr_office[0])*100\n",
        "pRMSE_LR_vs_Proposed_office=((lr_office[1]-proposed_method_office[1])/lr_office[1])*100\n",
        "pMAE_LR_vs_Proposed_office=((lr_office[2]-proposed_method_office[2])/lr_office[2])*100\n",
        "\n",
        "pMAPE_SVR_vs_Proposed_office=((svr_office[0]-proposed_method_office[0])/svr_office[0])*100\n",
        "pRMSE_SVR_vs_Proposed_office=((svr_office[1]-proposed_method_office[1])/svr_office[1])*100\n",
        "pMAE_SVR_vs_Proposed_office=((svr_office[2]-proposed_method_office[2])/svr_office[2])*100\n",
        "\n",
        "pMAPE_ANN_vs_Proposed_office=((ann_office[0]-proposed_method_office[0])/ann_office[0])*100\n",
        "pRMSE_ANN_vs_Proposed_office=((ann_office[1]-proposed_method_office[1])/ann_office[1])*100\n",
        "pMAE_ANN_vs_Proposed_office=((ann_office[2]-proposed_method_office[2])/ann_office[2])*100\n",
        "\n",
        "pMAPE_RF_vs_Proposed_office=((rf_office[0]-proposed_method_office[0])/rf_office[0])*100\n",
        "pRMSE_RF_vs_Proposed_office=((rf_office[1]-proposed_method_office[1])/rf_office[1])*100\n",
        "pMAE_RF_vs_Proposed_office=((rf_office[2]-proposed_method_office[2])/rf_office[2])*100\n",
        "\n",
        "pMAPE_LSTM_vs_Proposed_office=((lstm_office[0]-proposed_method_office[0])/lstm_office[0])*100\n",
        "pRMSE_LSTM_vs_Proposed_office=((lstm_office[1]-proposed_method_office[1])/lstm_office[1])*100\n",
        "pMAE_LSTM_vs_Proposed_office=((lstm_office[2]-proposed_method_office[2])/lstm_office[2])*100\n",
        "\n",
        "pMAPE_ceemdan_rf_vs_Proposed_office=((ceemdan_rf_office[0]-proposed_method_office[0])/ceemdan_rf_office[0])*100\n",
        "pRMSE_ceemdan_rf_vs_Proposed_office=((ceemdan_rf_office[1]-proposed_method_office[1])/ceemdan_rf_office[1])*100\n",
        "pMAE_ceemdan_rf_vs_Proposed_office=((ceemdan_rf_office[2]-proposed_method_office[2])/ceemdan_rf_office[2])*100\n",
        "\n",
        "\n",
        "pMAPE_ceemdan_lstm_vs_Proposed_office=((ceemdan_lstm_office[0]-proposed_method_office[0])/ceemdan_lstm_office[0])*100\n",
        "pRMSE_ceemdan_lstm_vs_Proposed_office=((ceemdan_lstm_office[1]-proposed_method_office[1])/ceemdan_lstm_office[1])*100\n",
        "pMAE_ceemdan_lstm_vs_Proposed_office=((ceemdan_lstm_office[2]-proposed_method_office[2])/ceemdan_lstm_office[2])*100\n",
        "\n",
        "\n",
        "df_PI_office=[[pMAPE_LR_vs_Proposed_office,pMAPE_SVR_vs_Proposed_office,pMAPE_ANN_vs_Proposed_office,\n",
        "                pMAPE_RF_vs_Proposed_office,pMAPE_LSTM_vs_Proposed_office,pMAPE_ceemdan_rf_vs_Proposed_office,\n",
        "                pMAPE_ceemdan_lstm_vs_Proposed_office],\n",
        "                [pRMSE_LR_vs_Proposed_office,pRMSE_SVR_vs_Proposed_office,pRMSE_ANN_vs_Proposed_office,\n",
        "                pRMSE_RF_vs_Proposed_office,pRMSE_LSTM_vs_Proposed_office,pRMSE_ceemdan_rf_vs_Proposed_office,\n",
        "                pRMSE_ceemdan_lstm_vs_Proposed_office],\n",
        "                [pMAE_LR_vs_Proposed_office,pMAE_SVR_vs_Proposed_office,pMAE_ANN_vs_Proposed_office,\n",
        "                pMAE_RF_vs_Proposed_office,pMAE_LSTM_vs_Proposed_office,pMAE_ceemdan_rf_vs_Proposed_office,\n",
        "                pMAE_ceemdan_lstm_vs_Proposed_office]]\n",
        "\n",
        "PI_office=pd.DataFrame(df_PI_office, columns=[\"Proposed Method vs.LR\", \"Proposed Method vs.SVR\",\" Proposed Method vs.ANN\",\n",
        "                                      \"Proposed Method vs.RF\",\"Proposed Method vs.LSTM\",\"Proposed Method vs.CEEMDAN RF\",\n",
        "                                      \"Proposed Method vs. CEEMDAN LSTM\"])\n",
        "PI_office= PI_office.round(decimals = 2)\n",
        "PI_office.set_axis(['MAPE(%)', 'RMSE','MAE'], axis='index')\n",
        "PI_office.style.set_caption(\"Percentage Improvement-Office Building\")\n",
        "index = PI_office.index\n",
        "index.name = \"Percentage Improvement office\"\n",
        "PI_office"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4ka5JKFdBmt"
      },
      "outputs": [],
      "source": [
        "#export table to png\n",
        "#dfi.export(PI_office,\"PI_office_table.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSQNapeAdBmt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU1DCoAcdBmt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}