{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmad162539/Smartbuilding/blob/main/University_Dormitory_Building.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSqrEpHafgnJ"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XklPTg0vfgnL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url_univdorm='https://raw.githubusercontent.com/Ahmad162539/Smartbuilding/main/data%20of%20UnivDorm_Prince.csv'\n",
        "univdorm= pd.read_csv(url_univdorm)\n",
        "data_univdorm= univdorm[(univdorm['timestamp'] > '2015-03-01') & (univdorm['timestamp'] < '2015-06-01')]\n",
        "dfs_univdorm=data_univdorm['energy']\n",
        "datas_univdorm=pd.DataFrame(dfs_univdorm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhdgGStjfgnM"
      },
      "source": [
        "## import libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import warnings\n",
        "\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "Si7oUPClgBFE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install EMD-signal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VwzD0YkgR5s",
        "outputId": "c7020e15-274a-4dd8-b71d-f6d042564f0b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting EMD-signal\n",
            "  Downloading EMD_signal-1.5.1-py3-none-any.whl (74 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.10/dist-packages (from EMD-signal) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from EMD-signal) (1.10.1)\n",
            "Collecting pathos>=0.2.1 (from EMD-signal)\n",
            "  Downloading pathos-0.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.64.* (from EMD-signal)\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ppft>=1.7.6.6 (from pathos>=0.2.1->EMD-signal)\n",
            "  Downloading ppft-1.7.6.6-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill>=0.3.6 (from pathos>=0.2.1->EMD-signal)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.2 (from pathos>=0.2.1->EMD-signal)\n",
            "  Downloading pox-0.3.2-py3-none-any.whl (29 kB)\n",
            "Collecting multiprocess>=0.70.14 (from pathos>=0.2.1->EMD-signal)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm, ppft, pox, dill, multiprocess, pathos, EMD-signal\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "Successfully installed EMD-signal-1.5.1 dill-0.3.6 multiprocess-0.70.14 pathos-0.3.0 pox-0.3.2 ppft-1.7.6.6 tqdm-4.64.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyEMD import CEEMDAN"
      ],
      "metadata": {
        "id": "8XBdEMXigD5c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-ny_zInQfgnM"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import warnings\n",
        "\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter('ignore')\n",
        "\n",
        "import numpy\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn import metrics\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dataframe_image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Y2zobEeiFqS",
        "outputId": "5d91f6ce-c52c-4286-c9da-d34b7af288bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dataframe_image\n",
            "  Downloading dataframe_image-0.1.11-py3-none-any.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (1.5.3)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (6.5.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (3.8.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (2.27.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (8.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (23.1)\n",
            "Requirement already satisfied: mistune in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (0.8.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from dataframe_image) (4.11.2)\n",
            "Collecting cssutils (from dataframe_image)\n",
            "  Downloading cssutils-2.7.1-py3-none-any.whl (399 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting html2image (from dataframe_image)\n",
            "  Downloading html2image-2.0.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (3.1.2)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (5.3.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (2.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (0.8.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (5.9.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (1.5.0)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (2.14.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (1.2.1)\n",
            "Requirement already satisfied: traitlets>=5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->dataframe_image) (5.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->dataframe_image) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->dataframe_image) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->dataframe_image) (1.22.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->dataframe_image) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->dataframe_image) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->dataframe_image) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->dataframe_image) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->dataframe_image) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->dataframe_image) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->dataframe_image) (1.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->dataframe_image) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dataframe_image) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dataframe_image) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dataframe_image) (3.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert>=5->dataframe_image) (3.8.1)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.5.0->nbconvert>=5->dataframe_image) (6.1.12)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert>=5->dataframe_image) (2.17.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert>=5->dataframe_image) (4.3.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.24->dataframe_image) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->dataframe_image) (0.5.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert>=5->dataframe_image) (0.19.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert>=5->dataframe_image) (23.2.1)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert>=5->dataframe_image) (6.3.1)\n",
            "Installing collected packages: html2image, cssutils, dataframe_image\n",
            "Successfully installed cssutils-2.7.1 dataframe_image-0.1.11 html2image-2.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dataframe_image as dfi"
      ],
      "metadata": {
        "id": "CRPP8RBKiL2S"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5archLmfgnM"
      },
      "source": [
        "## Import all functions from another notebook for building prediction methods"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "#Parameters Settings\n",
        "\n",
        "max_features=8 # the feature number of each node is set to 8, as it is suggested to be one-third of the feature's number (Lahouar and Slama,2017)\n",
        "lr=0.001 #learning rate LSTM as recommended in (Kingma and Ba, 2014)\n",
        "optimizer='Adam' #Adam is used as the optimizer for LSTM as it is computationally efficient (Dubey, Ashutosh Kumar, et al,2021)\n",
        "neuron=64\n",
        "epoch=100 #(Jorges et al, 2021)\n",
        "batch_size=64 #(Kandel et al,2020)\n",
        "n_hours=24 #in this study, to predict one hour ahead predicton, we use input from the previous 24 hour energy consumption\n",
        "data_partition=0.8 #in this study, we divided the data into 80% of data as training data and 20% of the data as testing data"
      ],
      "metadata": {
        "id": "bK4BzjJZmaJ8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from PyEMD import CEEMDAN\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "### import the libraries\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from math import sqrt\n",
        "\n",
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back, 0])\n",
        "    return numpy.array(dataX), numpy.array(dataY)\n",
        "\n",
        "def percentage_error(actual, predicted):\n",
        "    res = numpy.empty(actual.shape)\n",
        "    for j in range(actual.shape[0]):\n",
        "        if actual[j] != 0:\n",
        "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
        "        else:\n",
        "            res[j] = predicted[j] / np.mean(actual)\n",
        "    return res\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return numpy.mean(numpy.abs(percentage_error(numpy.asarray(y_true), numpy.asarray(y_pred)))) * 100\n",
        "\n",
        "\n",
        "\n",
        "# In[25]:\n",
        "\n",
        "\n",
        "def lr_model(datass,look_back,data_partition):\n",
        "\n",
        "    datasets=datass.values\n",
        "    train_size = int(len(datasets) * data_partition)\n",
        "    test_size = len(datasets) - train_size\n",
        "    train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "\n",
        "    trainX, trainY = create_dataset(train, look_back)\n",
        "    testX, testY = create_dataset(test, look_back)\n",
        "    X_train=pd.DataFrame(trainX)\n",
        "    Y_train=pd.DataFrame(trainY)\n",
        "    X_test=pd.DataFrame(testX)\n",
        "    Y_test=pd.DataFrame(testY)\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "\n",
        "    X= sc_X.fit_transform(X_train)\n",
        "    y= sc_y.fit_transform(Y_train)\n",
        "    X1= sc_X.fit_transform(X_test)\n",
        "    y1= sc_y.fit_transform(Y_test)\n",
        "    y=y.ravel()\n",
        "    y1=y1.ravel()\n",
        "    import tensorflow as tf\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "    tf.random.set_seed(1234)\n",
        "\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "\n",
        "    grid = LinearRegression()\n",
        "\n",
        "    grid.fit(X,y)\n",
        "    y_pred_train_lr= grid.predict(X)\n",
        "    y_pred_test_lr= grid.predict(X1)\n",
        "\n",
        "    y_pred_train_lr=pd.DataFrame(y_pred_train_lr)\n",
        "    y_pred_test_lr=pd.DataFrame(y_pred_test_lr)\n",
        "\n",
        "    y1=pd.DataFrame(y1)\n",
        "    y=pd.DataFrame(y)\n",
        "\n",
        "\n",
        "    y_pred_test1_lr= sc_y.inverse_transform (y_pred_test_lr)\n",
        "    y_pred_train1_lr=sc_y.inverse_transform (y_pred_train_lr)\n",
        "\n",
        "    y_test= sc_y.inverse_transform (y1)\n",
        "    y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "    y_pred_test1_rf=pd.DataFrame(y_pred_test1_lr)\n",
        "    y_pred_train1_rf=pd.DataFrame(y_pred_train1_lr)\n",
        "\n",
        "    y_test= pd.DataFrame(y_test)\n",
        "\n",
        "\n",
        "    #summarize the fit of the model\n",
        "    mape=mean_absolute_percentage_error(y_test,y_pred_test1_lr)\n",
        "    rmse= sqrt(mean_squared_error(y_test,y_pred_test1_lr))\n",
        "    mae=metrics.mean_absolute_error(y_test,y_pred_test1_lr)\n",
        "\n",
        "\n",
        "    return mape,rmse,mae\n",
        "\n",
        "\n",
        "# In[26]:\n",
        "\n",
        "\n",
        "def svr_model(datass,look_back,data_partition):\n",
        "\n",
        "    datasets=datass.values\n",
        "    train_size = int(len(datasets) * data_partition)\n",
        "    test_size = len(datasets) - train_size\n",
        "    train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "\n",
        "    trainX, trainY = create_dataset(train, look_back)\n",
        "    testX, testY = create_dataset(test, look_back)\n",
        "    X_train=pd.DataFrame(trainX)\n",
        "    Y_train=pd.DataFrame(trainY)\n",
        "    X_test=pd.DataFrame(testX)\n",
        "    Y_test=pd.DataFrame(testY)\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "\n",
        "    X= sc_X.fit_transform(X_train)\n",
        "    y= sc_y.fit_transform(Y_train)\n",
        "    X1= sc_X.fit_transform(X_test)\n",
        "    y1= sc_y.fit_transform(Y_test)\n",
        "    y=y.ravel()\n",
        "    y1=y1.ravel()\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "\n",
        "    import tensorflow as tf\n",
        "    tf.random.set_seed(1234)\n",
        "\n",
        "\n",
        "    from sklearn.svm import SVR\n",
        "\n",
        "    grid = SVR()\n",
        "    grid.fit(X,y)\n",
        "    y_pred_train_svr= grid.predict(X)\n",
        "    y_pred_test_svr= grid.predict(X1)\n",
        "\n",
        "    y_pred_train_svr=pd.DataFrame(y_pred_train_svr)\n",
        "    y_pred_test_svr=pd.DataFrame(y_pred_test_svr)\n",
        "\n",
        "    y1=pd.DataFrame(y1)\n",
        "    y=pd.DataFrame(y)\n",
        "\n",
        "\n",
        "    y_pred_test1_svr= sc_y.inverse_transform (y_pred_test_svr)\n",
        "    y_pred_train1_svr=sc_y.inverse_transform (y_pred_train_svr)\n",
        "\n",
        "    y_test= sc_y.inverse_transform (y1)\n",
        "    y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "    y_pred_test1_svr=pd.DataFrame(y_pred_test1_svr)\n",
        "    y_pred_train1_svr=pd.DataFrame(y_pred_train1_svr)\n",
        "\n",
        "    y_test= pd.DataFrame(y_test)\n",
        "\n",
        "\n",
        "    #summarize the fit of the model\n",
        "    mape=mean_absolute_percentage_error(y_test,y_pred_test1_svr)\n",
        "    rmse= sqrt(mean_squared_error(y_test,y_pred_test1_svr))\n",
        "    mae=metrics.mean_absolute_error(y_test,y_pred_test1_svr)\n",
        "\n",
        "\n",
        "    return mape,rmse,mae\n",
        "\n",
        "\n",
        "# In[27]:\n",
        "\n",
        "\n",
        "\n",
        "def ann_model(datass,look_back,data_partition):\n",
        "\n",
        "\n",
        "    datasets=datass.values\n",
        "    train_size = int(len(datasets) * data_partition)\n",
        "    test_size = len(datasets) - train_size\n",
        "    train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "    trainX, trainY = create_dataset(train, look_back)\n",
        "    testX, testY = create_dataset(test, look_back)\n",
        "    X_train=pd.DataFrame(trainX)\n",
        "    Y_train=pd.DataFrame(trainY)\n",
        "    X_test=pd.DataFrame(testX)\n",
        "    Y_test=pd.DataFrame(testY)\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "\n",
        "    X= sc_X.fit_transform(X_train)\n",
        "    y= sc_y.fit_transform(Y_train)\n",
        "    X1= sc_X.fit_transform(X_test)\n",
        "    y1= sc_y.fit_transform(Y_test)\n",
        "    y=y.ravel()\n",
        "    y1=y1.ravel()\n",
        "\n",
        "    import numpy\n",
        "\n",
        "    trainX = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
        "    testX = numpy.reshape(X1, (X1.shape[0], 1, X1.shape[1]))\n",
        "\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "    import tensorflow as tf\n",
        "    tf.random.set_seed(1234)\n",
        "\n",
        "\n",
        "    from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "    model= MLPRegressor(random_state=1,activation='tanh').fit(X,y)\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "\n",
        "\n",
        "    # make predictions\n",
        "    y_pred_train = model.predict(X)\n",
        "    y_pred_test = model.predict(X1)\n",
        "    y_pred_test= numpy.array(y_pred_test).ravel()\n",
        "\n",
        "    y_pred_test=pd.DataFrame(y_pred_test)\n",
        "    y1=pd.DataFrame(y1)\n",
        "\n",
        "    y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
        "    y_test= sc_y.inverse_transform (y1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #summarize the fit of the model\n",
        "    mape=mean_absolute_percentage_error(y_test,y_pred_test1)\n",
        "    rmse= sqrt(mean_squared_error(y_test,y_pred_test1))\n",
        "    mae=metrics.mean_absolute_error(y_test,y_pred_test1)\n",
        "\n",
        "\n",
        "    return mape,rmse,mae\n",
        "\n",
        "\n",
        "# In[28]:\n",
        "\n",
        "\n",
        "def rf_model(datass,look_back,data_partition,max_features):\n",
        "\n",
        "    datasets=datass.values\n",
        "    train_size = int(len(datasets) * data_partition)\n",
        "    test_size = len(datasets) - train_size\n",
        "    train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "\n",
        "    trainX, trainY = create_dataset(train, look_back)\n",
        "    testX, testY = create_dataset(test, look_back)\n",
        "    X_train=pd.DataFrame(trainX)\n",
        "    Y_train=pd.DataFrame(trainY)\n",
        "    X_test=pd.DataFrame(testX)\n",
        "    Y_test=pd.DataFrame(testY)\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "\n",
        "    X= sc_X.fit_transform(X_train)\n",
        "    y= sc_y.fit_transform(Y_train)\n",
        "    X1= sc_X.fit_transform(X_test)\n",
        "    y1= sc_y.fit_transform(Y_test)\n",
        "    y=y.ravel()\n",
        "    y1=y1.ravel()\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "    import tensorflow as tf\n",
        "    tf.random.set_seed(1234)\n",
        "\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "    grid = RandomForestRegressor(max_features=max_features)\n",
        "    grid.fit(X,y)\n",
        "    y_pred_train_rf= grid.predict(X)\n",
        "    y_pred_test_rf= grid.predict(X1)\n",
        "\n",
        "    y_pred_train_rf=pd.DataFrame(y_pred_train_rf)\n",
        "    y_pred_test_rf=pd.DataFrame(y_pred_test_rf)\n",
        "\n",
        "    y1=pd.DataFrame(y1)\n",
        "    y=pd.DataFrame(y)\n",
        "\n",
        "\n",
        "    y_pred_test1_rf= sc_y.inverse_transform (y_pred_test_rf)\n",
        "    y_pred_train1_rf=sc_y.inverse_transform (y_pred_train_rf)\n",
        "\n",
        "    y_test= sc_y.inverse_transform (y1)\n",
        "    y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "    y_pred_test1_rf=pd.DataFrame(y_pred_test1_rf)\n",
        "    y_pred_train1_rf=pd.DataFrame(y_pred_train1_rf)\n",
        "\n",
        "    y_test= pd.DataFrame(y_test)\n",
        "\n",
        "\n",
        "    #summarize the fit of the model\n",
        "    mape=mean_absolute_percentage_error(y_test,y_pred_test1_rf)\n",
        "    rmse= sqrt(mean_squared_error(y_test,y_pred_test1_rf))\n",
        "    mae=metrics.mean_absolute_error(y_test,y_pred_test1_rf)\n",
        "\n",
        "\n",
        "    return mape,rmse,mae\n",
        "\n",
        "\n",
        "# In[29]:\n",
        "\n",
        "\n",
        "def lstm_model(datass,look_back,data_partition,max_features,epoch,batch_size,neuron,lr,optimizer):\n",
        "    datasets=datass.values\n",
        "    train_size = int(len(datasets) * data_partition)\n",
        "    test_size = len(datasets) - train_size\n",
        "    train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "    trainX, trainY = create_dataset(train, look_back)\n",
        "    testX, testY = create_dataset(test, look_back)\n",
        "    X_train=pd.DataFrame(trainX)\n",
        "    Y_train=pd.DataFrame(trainY)\n",
        "    X_test=pd.DataFrame(testX)\n",
        "    Y_test=pd.DataFrame(testY)\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "\n",
        "    X= sc_X.fit_transform(X_train)\n",
        "    y= sc_y.fit_transform(Y_train)\n",
        "    X1= sc_X.fit_transform(X_test)\n",
        "    y1= sc_y.fit_transform(Y_test)\n",
        "    y=y.ravel()\n",
        "    y1=y1.ravel()\n",
        "    trainX1 = numpy.reshape(X, (X.shape[0],1,X.shape[1]))\n",
        "    testX1 = numpy.reshape(X1, (X1.shape[0],1,X1.shape[1]))\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "    import tensorflow as tf\n",
        "    tf.random.set_seed(1234)\n",
        "\n",
        "    import os\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers.core import Dense, Dropout, Activation\n",
        "    from keras.layers import LSTM\n",
        "\n",
        "\n",
        "    neuron=neuron\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units = neuron,input_shape=(trainX1.shape[1], trainX1.shape[2])))\n",
        "    model.add(Dense(1))\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(loss='mse',optimizer=optimizer)\n",
        "#    model.summary()\n",
        "\n",
        "\n",
        "  # Fitting the RNN to the Training s\n",
        "    model.fit(trainX1, y, epochs = epoch, batch_size = batch_size,verbose=0)\n",
        "  # make predictions\n",
        "    y_pred_train = model.predict(trainX1)\n",
        "    y_pred_test = model.predict(testX1)\n",
        "    y_pred_test= numpy.array(y_pred_test).ravel()\n",
        "\n",
        "    y_pred_test=pd.DataFrame(y_pred_test)\n",
        "    y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
        "    y1=pd.DataFrame(y1)\n",
        "\n",
        "    y_test= sc_y.inverse_transform (y1)\n",
        "\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "    from sklearn import metrics\n",
        "\n",
        "    mape=mean_absolute_percentage_error(y_test,y_pred_test1)\n",
        "    rmse= sqrt(mean_squared_error(y_test,y_pred_test1))\n",
        "    mae=metrics.mean_absolute_error(y_test,y_pred_test1)\n",
        "    return mape,rmse,mae\n",
        "\n",
        "\n",
        "# In[30]:\n",
        "\n",
        "\n",
        "###################################################hybrid based ceemdan####################################################\n",
        "def hybrid_ceemdan_rf(datass,look_back,data_partition,max_features):\n",
        "\n",
        "\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "\n",
        "    dfs=datass\n",
        "    s = dfs.values\n",
        "\n",
        "    emd = CEEMDAN(epsilon=0.05)\n",
        "    emd.noise_seed(12345)\n",
        "\n",
        "    IMFs = emd(s)\n",
        "\n",
        "\n",
        "    full_imf=pd.DataFrame(IMFs)\n",
        "    data_imf=full_imf.T\n",
        "\n",
        "    import pandas as pd\n",
        "\n",
        "    pred_test=[]\n",
        "    test_ori=[]\n",
        "    pred_train=[]\n",
        "    train_ori=[]\n",
        "\n",
        "\n",
        "    for col in data_imf:\n",
        "\n",
        "        datasetss2=pd.DataFrame(data_imf[col])\n",
        "        datasets=datasetss2.values\n",
        "        train_size = int(len(datasets) * data_partition)\n",
        "        test_size = len(datasets) - train_size\n",
        "        train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "\n",
        "        trainX, trainY = create_dataset(train, look_back)\n",
        "        testX, testY = create_dataset(test, look_back)\n",
        "        X_train=pd.DataFrame(trainX)\n",
        "        Y_train=pd.DataFrame(trainY)\n",
        "        X_test=pd.DataFrame(testX)\n",
        "        Y_test=pd.DataFrame(testY)\n",
        "        sc_X = StandardScaler()\n",
        "        sc_y = StandardScaler()\n",
        "        X= sc_X.fit_transform(X_train)\n",
        "        y= sc_y.fit_transform(Y_train)\n",
        "        X1= sc_X.fit_transform(X_test)\n",
        "        y1= sc_y.fit_transform(Y_test)\n",
        "        y=y.ravel()\n",
        "        y1=y1.ravel()\n",
        "\n",
        "        import numpy\n",
        "\n",
        "        numpy.random.seed(1234)\n",
        "        import tensorflow as tf\n",
        "        tf.random.set_seed(1234)\n",
        "\n",
        "\n",
        "        from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "        grid = RandomForestRegressor(max_features=max_features)\n",
        "        grid.fit(X,y)\n",
        "        y_pred_train= grid.predict(X)\n",
        "        y_pred_test= grid.predict(X1)\n",
        "\n",
        "        y_pred_test=pd.DataFrame(y_pred_test)\n",
        "        y_pred_train=pd.DataFrame(y_pred_train)\n",
        "\n",
        "        y1=pd.DataFrame(y1)\n",
        "        y=pd.DataFrame(y)\n",
        "\n",
        "        y_test= sc_y.inverse_transform (y1)\n",
        "        y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "        y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
        "        y_pred_train1= sc_y.inverse_transform (y_pred_train)\n",
        "\n",
        "\n",
        "        pred_test.append(y_pred_test1)\n",
        "        test_ori.append(y_test)\n",
        "        pred_train.append(y_pred_train1)\n",
        "        train_ori.append(y_train)\n",
        "\n",
        "\n",
        "    result_pred_test= pd.DataFrame.from_records(pred_test)\n",
        "    result_pred_train= pd.DataFrame.from_records(pred_train)\n",
        "\n",
        "\n",
        "    a=result_pred_test.sum(axis = 0, skipna = True)\n",
        "    b=result_pred_train.sum(axis = 0, skipna = True)\n",
        "\n",
        "\n",
        "    dataframe=pd.DataFrame(dfs)\n",
        "    dataset=dataframe.values\n",
        "\n",
        "    train_size = int(len(dataset) * data_partition)\n",
        "    test_size = len(dataset) - train_size\n",
        "    train, test = dataset[0:train_size], dataset[train_size:len(dataset)]\n",
        "\n",
        "    trainX, trainY = create_dataset(train, look_back)\n",
        "    testX, testY = create_dataset(test, look_back)\n",
        "    X_train=pd.DataFrame(trainX)\n",
        "    Y_train=pd.DataFrame(trainY)\n",
        "    X_test=pd.DataFrame(testX)\n",
        "    Y_test=pd.DataFrame(testY)\n",
        "\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "    X= sc_X.fit_transform(X_train)\n",
        "    y= sc_y.fit_transform(Y_train)\n",
        "    X1= sc_X.fit_transform(X_test)\n",
        "    y1= sc_y.fit_transform(Y_test)\n",
        "    y=y.ravel()\n",
        "    y1=y1.ravel()\n",
        "\n",
        "\n",
        "    trainX = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
        "    testX = numpy.reshape(X1, (X1.shape[0], 1, X1.shape[1]))\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "    import tensorflow as tf\n",
        "    tf.random.set_seed(1234)\n",
        "\n",
        "    y1=pd.DataFrame(y1)\n",
        "    y=pd.DataFrame(y)\n",
        "    y_test= sc_y.inverse_transform (y1)\n",
        "    y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    a= pd.DataFrame(a)\n",
        "    y_test= pd.DataFrame(y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   #summarize the fit of the model\n",
        "    mape=mean_absolute_percentage_error(y_test,a)\n",
        "    rmse= sqrt(mean_squared_error(y_test,a))\n",
        "    mae=metrics.mean_absolute_error(y_test,a)\n",
        "\n",
        "\n",
        "\n",
        "    return mape,rmse,mae\n",
        "\n",
        "\n",
        "# In[31]:\n",
        "\n",
        "\n",
        "def hybrid_ceemdan_lstm(datass,look_back,data_partition,max_features,epoch,batch_size,neuron,lr,optimizer):\n",
        "\n",
        "\n",
        "    from PyEMD import CEEMDAN\n",
        "\n",
        "    dfs=datass\n",
        "    s = dfs.values\n",
        "\n",
        "    emd = CEEMDAN(epsilon=0.05)\n",
        "    emd.noise_seed(12345)\n",
        "\n",
        "    IMFs = emd(s)\n",
        "\n",
        "\n",
        "    full_imf=pd.DataFrame(IMFs)\n",
        "    data_imf=full_imf.T\n",
        "\n",
        "    pred_test=[]\n",
        "    test_ori=[]\n",
        "    pred_train=[]\n",
        "    train_ori=[]\n",
        "\n",
        "\n",
        "    for col in data_imf:\n",
        "\n",
        "        datasetss2=pd.DataFrame(data_imf[col])\n",
        "        datasets=datasetss2.values\n",
        "        train_size = int(len(datasets) * data_partition)\n",
        "        test_size = len(datasets) - train_size\n",
        "        train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "\n",
        "        trainX, trainY = create_dataset(train, look_back)\n",
        "        testX, testY = create_dataset(test, look_back)\n",
        "        X_train=pd.DataFrame(trainX)\n",
        "        Y_train=pd.DataFrame(trainY)\n",
        "        X_test=pd.DataFrame(testX)\n",
        "        Y_test=pd.DataFrame(testY)\n",
        "        sc_X = StandardScaler()\n",
        "        sc_y = StandardScaler()\n",
        "        X= sc_X.fit_transform(X_train)\n",
        "        y= sc_y.fit_transform(Y_train)\n",
        "        X1= sc_X.fit_transform(X_test)\n",
        "        y1= sc_y.fit_transform(Y_test)\n",
        "        y=y.ravel()\n",
        "        y1=y1.ravel()\n",
        "\n",
        "        import numpy\n",
        "\n",
        "        trainX = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
        "        testX = numpy.reshape(X1, (X1.shape[0], 1, X1.shape[1]))\n",
        "\n",
        "        numpy.random.seed(1234)\n",
        "        import tensorflow as tf\n",
        "        tf.random.set_seed(1234)\n",
        "\n",
        "\n",
        "        from keras.models import Sequential\n",
        "        from keras.layers.core import Dense, Dropout, Activation\n",
        "        from keras.layers import LSTM\n",
        "\n",
        "\n",
        "        neuron=neuron\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(units = neuron,input_shape=(trainX.shape[1], trainX.shape[2])))\n",
        "        model.add(Dense(1))\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        model.compile(loss='mse',optimizer=optimizer)\n",
        "\n",
        "        numpy.random.seed(1234)\n",
        "\n",
        "\n",
        "        # Fitting the RNN to the Training set\n",
        "        model.fit(trainX, y, epochs = epoch, batch_size = batch_size,verbose=0)\n",
        "\n",
        "        # make predictions\n",
        "        y_pred_train = model.predict(trainX)\n",
        "        y_pred_test = model.predict(testX)\n",
        "\n",
        "        # make predictions\n",
        "\n",
        "        y_pred_test= numpy.array(y_pred_test).ravel()\n",
        "        y_pred_test=pd.DataFrame(y_pred_test)\n",
        "        y1=pd.DataFrame(y1)\n",
        "        y=pd.DataFrame(y)\n",
        "        y_pred_train= numpy.array(y_pred_train).ravel()\n",
        "        y_pred_train=pd.DataFrame(y_pred_train)\n",
        "\n",
        "\n",
        "        y_test= sc_y.inverse_transform (y1)\n",
        "        y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "        y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
        "        y_pred_train1= sc_y.inverse_transform (y_pred_train)\n",
        "\n",
        "\n",
        "        pred_test.append(y_pred_test1)\n",
        "        test_ori.append(y_test)\n",
        "        pred_train.append(y_pred_train1)\n",
        "        train_ori.append(y_train)\n",
        "\n",
        "\n",
        "    result_pred_test= pd.DataFrame.from_records(pred_test)\n",
        "    result_pred_train= pd.DataFrame.from_records(pred_train)\n",
        "\n",
        "\n",
        "    a=result_pred_test.sum(axis = 0, skipna = True)\n",
        "    b=result_pred_train.sum(axis = 0, skipna = True)\n",
        "\n",
        "\n",
        "    dataframe=pd.DataFrame(dfs)\n",
        "    dataset=dataframe.values\n",
        "\n",
        "    train_size = int(len(dataset) * data_partition)\n",
        "    test_size = len(dataset) - train_size\n",
        "    train, test = dataset[0:train_size], dataset[train_size:len(dataset)]\n",
        "\n",
        "    trainX, trainY = create_dataset(train, look_back)\n",
        "    testX, testY = create_dataset(test, look_back)\n",
        "    X_train=pd.DataFrame(trainX)\n",
        "    Y_train=pd.DataFrame(trainY)\n",
        "    X_test=pd.DataFrame(testX)\n",
        "    Y_test=pd.DataFrame(testY)\n",
        "\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "    X= sc_X.fit_transform(X_train)\n",
        "    y= sc_y.fit_transform(Y_train)\n",
        "    X1= sc_X.fit_transform(X_test)\n",
        "    y1= sc_y.fit_transform(Y_test)\n",
        "    y=y.ravel()\n",
        "    y1=y1.ravel()\n",
        "\n",
        "\n",
        "    trainX = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
        "    testX = numpy.reshape(X1, (X1.shape[0], 1, X1.shape[1]))\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "    import tensorflow as tf\n",
        "    tf.random.set_seed(1234)\n",
        "\n",
        "    y1=pd.DataFrame(y1)\n",
        "    y=pd.DataFrame(y)\n",
        "\n",
        "    y_test= sc_y.inverse_transform (y1)\n",
        "    y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "\n",
        "    a= pd.DataFrame(a)\n",
        "    y_test= pd.DataFrame(y_test)\n",
        "\n",
        "\n",
        "\n",
        "   #summarize the fit of the model\n",
        "    mape=mean_absolute_percentage_error(y_test,a)\n",
        "    rmse= sqrt(mean_squared_error(y_test,a))\n",
        "    mae=metrics.mean_absolute_error(y_test,a)\n",
        "\n",
        "    return mape,rmse,mae\n",
        "\n",
        "\n",
        "# In[32]:\n",
        "\n",
        "\n",
        "def proposed_method(datass,look_back,data_partition,max_features,epoch,batch_size,neuron,lr,optimizer):\n",
        "\n",
        "    from PyEMD import CEEMDAN\n",
        "\n",
        "\n",
        "    dfs=datass\n",
        "    s = dfs.values\n",
        "\n",
        "    emd = CEEMDAN(epsilon=0.05)\n",
        "    emd.noise_seed(12345)\n",
        "\n",
        "    IMFs = emd(s)\n",
        "\n",
        "\n",
        "    full_imf=pd.DataFrame(IMFs)\n",
        "    data_imf=full_imf.T\n",
        "\n",
        "\n",
        "\n",
        "    pred_test=[]\n",
        "    test_ori=[]\n",
        "    pred_train=[]\n",
        "    train_ori=[]\n",
        "\n",
        "\n",
        "    n_imf=len(data_imf.columns)\n",
        "\n",
        "    k=list(range(1,n_imf))\n",
        "    m=[0]\n",
        "\n",
        "\n",
        "    for i in m:\n",
        "\n",
        "        datasetss2=pd.DataFrame(data_imf[i])\n",
        "        datasets=datasetss2.values\n",
        "        train_size = int(len(datasets) * data_partition)\n",
        "        test_size = len(datasets) - train_size\n",
        "        train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "\n",
        "        trainX, trainY = create_dataset(train, look_back)\n",
        "        testX, testY = create_dataset(test, look_back)\n",
        "        X_train=pd.DataFrame(trainX)\n",
        "        Y_train=pd.DataFrame(trainY)\n",
        "        X_test=pd.DataFrame(testX)\n",
        "        Y_test=pd.DataFrame(testY)\n",
        "        sc_X = StandardScaler()\n",
        "        sc_y = StandardScaler()\n",
        "        X= sc_X.fit_transform(X_train)\n",
        "        y= sc_y.fit_transform(Y_train)\n",
        "        X1= sc_X.fit_transform(X_test)\n",
        "        y1= sc_y.fit_transform(Y_test)\n",
        "        y=y.ravel()\n",
        "        y1=y1.ravel()\n",
        "\n",
        "        import numpy\n",
        "\n",
        "        trainX = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
        "        testX = numpy.reshape(X1, (X1.shape[0], 1, X1.shape[1]))\n",
        "\n",
        "        numpy.random.seed(1234)\n",
        "        import tensorflow as tf\n",
        "        tf.random.set_seed(1234)\n",
        "\n",
        "\n",
        "        from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "        grid = RandomForestRegressor(max_features=max_features)\n",
        "        grid.fit(X,y)\n",
        "        y_pred_train= grid.predict(X)\n",
        "        y_pred_test= grid.predict(X1)\n",
        "\n",
        "        y_pred_test=pd.DataFrame(y_pred_test)\n",
        "        y_pred_train=pd.DataFrame(y_pred_train)\n",
        "\n",
        "        y1=pd.DataFrame(y1)\n",
        "        y=pd.DataFrame(y)\n",
        "\n",
        "        y_test= sc_y.inverse_transform (y1)\n",
        "        y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "        y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
        "        y_pred_train1= sc_y.inverse_transform (y_pred_train)\n",
        "\n",
        "\n",
        "        pred_test.append(y_pred_test1)\n",
        "        test_ori.append(y_test)\n",
        "        pred_train.append(y_pred_train1)\n",
        "        train_ori.append(y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for i in k:\n",
        "\n",
        "        datasetss2=pd.DataFrame(data_imf[i])\n",
        "        datasets=datasetss2.values\n",
        "        train_size = int(len(datasets) * data_partition)\n",
        "        test_size = len(datasets) - train_size\n",
        "        train, test = datasets[0:train_size], datasets[train_size:len(datasets)]\n",
        "\n",
        "        trainX, trainY = create_dataset(train, look_back)\n",
        "        testX, testY = create_dataset(test, look_back)\n",
        "        X_train=pd.DataFrame(trainX)\n",
        "        Y_train=pd.DataFrame(trainY)\n",
        "        X_test=pd.DataFrame(testX)\n",
        "        Y_test=pd.DataFrame(testY)\n",
        "        sc_X = StandardScaler()\n",
        "        sc_y = StandardScaler()\n",
        "        X= sc_X.fit_transform(X_train)\n",
        "        y= sc_y.fit_transform(Y_train)\n",
        "        X1= sc_X.fit_transform(X_test)\n",
        "        y1= sc_y.fit_transform(Y_test)\n",
        "        y=y.ravel()\n",
        "        y1=y1.ravel()\n",
        "\n",
        "        import numpy\n",
        "\n",
        "        trainX = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
        "        testX = numpy.reshape(X1, (X1.shape[0], 1, X1.shape[1]))\n",
        "\n",
        "        numpy.random.seed(1234)\n",
        "        import tensorflow as tf\n",
        "        tf.random.set_seed(1234)\n",
        "\n",
        "\n",
        "        from keras.models import Sequential\n",
        "        from keras.layers.core import Dense, Dropout, Activation\n",
        "        from keras.layers import LSTM\n",
        "\n",
        "        neuron=neuron\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(units = neuron,input_shape=(trainX.shape[1], trainX.shape[2])))\n",
        "        model.add(Dense(1))\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        model.compile(loss='mse',optimizer=optimizer)\n",
        "\n",
        "\n",
        "        numpy.random.seed(1234)\n",
        "\n",
        "\n",
        "     # Fitting the RNN to the Training set\n",
        "        model.fit(trainX, y, epochs = epoch, batch_size = batch_size,verbose=0)\n",
        "\n",
        "\n",
        "        # make predictions\n",
        "        y_pred_train = model.predict(trainX)\n",
        "        y_pred_test = model.predict(testX)\n",
        "\n",
        "        # make predictions\n",
        "\n",
        "\n",
        "        y_pred_test= numpy.array(y_pred_test).ravel()\n",
        "        y_pred_test=pd.DataFrame(y_pred_test)\n",
        "        y1=pd.DataFrame(y1)\n",
        "        y=pd.DataFrame(y)\n",
        "        y_pred_train= numpy.array(y_pred_train).ravel()\n",
        "        y_pred_train=pd.DataFrame(y_pred_train)\n",
        "\n",
        "\n",
        "        y_test= sc_y.inverse_transform (y1)\n",
        "        y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "        y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
        "        y_pred_train1= sc_y.inverse_transform (y_pred_train)\n",
        "\n",
        "\n",
        "        pred_test.append(y_pred_test1)\n",
        "        test_ori.append(y_test)\n",
        "        pred_train.append(y_pred_train1)\n",
        "        train_ori.append(y_train)\n",
        "\n",
        "    result_pred_test= pd.DataFrame.from_records(pred_test)\n",
        "    result_pred_train= pd.DataFrame.from_records(pred_train)\n",
        "\n",
        "\n",
        "    a=result_pred_test.sum(axis = 0, skipna = True)\n",
        "    b=result_pred_train.sum(axis = 0, skipna = True)\n",
        "\n",
        "\n",
        "    dataframe=pd.DataFrame(dfs)\n",
        "    dataset=dataframe.values\n",
        "\n",
        "    train_size = int(len(dataset) * data_partition)\n",
        "    test_size = len(dataset) - train_size\n",
        "    train, test = dataset[0:train_size], dataset[train_size:len(dataset)]\n",
        "\n",
        "    trainX, trainY = create_dataset(train, look_back)\n",
        "    testX, testY = create_dataset(test, look_back)\n",
        "    X_train=pd.DataFrame(trainX)\n",
        "    Y_train=pd.DataFrame(trainY)\n",
        "    X_test=pd.DataFrame(testX)\n",
        "    Y_test=pd.DataFrame(testY)\n",
        "\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "    X= sc_X.fit_transform(X_train)\n",
        "    y= sc_y.fit_transform(Y_train)\n",
        "    X1= sc_X.fit_transform(X_test)\n",
        "    y1= sc_y.fit_transform(Y_test)\n",
        "    y=y.ravel()\n",
        "    y1=y1.ravel()\n",
        "\n",
        "\n",
        "    trainX = numpy.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
        "    testX = numpy.reshape(X1, (X1.shape[0], 1, X1.shape[1]))\n",
        "\n",
        "    numpy.random.seed(1234)\n",
        "    import tensorflow as tf\n",
        "    tf.random.set_seed(1234)\n",
        "\n",
        "    y1=pd.DataFrame(y1)\n",
        "    y=pd.DataFrame(y)\n",
        "    y_test= sc_y.inverse_transform (y1)\n",
        "    y_train= sc_y.inverse_transform (y)\n",
        "\n",
        "\n",
        "    a= pd.DataFrame(a)\n",
        "    y_test= pd.DataFrame(y_test)\n",
        "\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "\n",
        "   #summarize the fit of the model\n",
        "    mape=mean_absolute_percentage_error(y_test,a)\n",
        "    rmse= sqrt(mean_squared_error(y_test,a))\n",
        "    mae=metrics.mean_absolute_error(y_test,a)\n",
        "\n",
        "    return mape,rmse,mae,a,y_test\n"
      ],
      "metadata": {
        "id": "TzduYmntmwj7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8ju1llJfgnN"
      },
      "source": [
        "## Import parameter settings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_features=8 # the feature number of each node is set to 8, as it is suggested to be one-third of the feature's number (Lahouar and Slama,2017)\n",
        "lr=0.001 #learning rate LSTM as recommended in (Kingma and Ba, 2014)\n",
        "optimizer='Adam' #Adam is used as the optimizer for LSTM as it is computationally efficient (Dubey, Ashutosh Kumar, et al,2021)\n",
        "neuron=64\n",
        "epoch=100 #(Jorges et al, 2021)\n",
        "batch_size=64 #(Kandel et al,2020)\n",
        "hours=24 #in this study, to predict one hour ahead predicton, we use input from the previous 24 hour energy consumption\n",
        "data_partition=0.8 #in this study, we divided the data into 80% of data as training data and 20% of the data as testing data"
      ],
      "metadata": {
        "id": "_abXl6B6nVMd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwFutNkWfgnN"
      },
      "source": [
        "## Run the experiments\n",
        "### Run this following cell will train and test the proposed method and other benchmark methods on University Dormitory Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoBT7K_5oP-Y",
        "outputId": "7f50d80e-b89e-4cc8-ae5f-9ed46299fd7f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM"
      ],
      "metadata": {
        "id": "zuHWMkP1qCGi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaTyglZ2fgnN",
        "outputId": "4b9d1913-d8d2-4814-bda6-a80beadf4d14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 0.3751866817474365 seconds - Linear Regression- univdorm ---\n",
            "--- 1.036478042602539 seconds - Support Vector Regression- univdorm ---\n",
            "--- 15.30004620552063 seconds - ANN- univdorm ---\n",
            "--- 3.610328197479248 seconds - Random Forest- univdorm ---\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "--- 26.363741159439087 seconds - lstm- univdorm ---\n",
            "--- 33.78251552581787 seconds - ceemdan_rf- univdorm ---\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "--- 140.67435908317566 seconds - ceemdan_lstm- univdorm ---\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 0s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 1s 2ms/step\n",
            "14/14 [==============================] - 0s 2ms/step\n",
            "55/55 [==============================] - 1s 3ms/step\n",
            "14/14 [==============================] - 0s 3ms/step\n",
            "--- 128.83940410614014 seconds - proposed_method- univdorm ---\n"
          ]
        }
      ],
      "source": [
        "#Linear Regression\n",
        "\n",
        "start_time = time.time()\n",
        "lr_univdorm=lr_model(datas_univdorm,hours,data_partition)\n",
        "lr_time_univdorm=time.time() - start_time\n",
        "print(\"--- %s seconds - Linear Regression- univdorm ---\" % (lr_time_univdorm))\n",
        "\n",
        "#Support Vector Regression\n",
        "start_time = time.time()\n",
        "svr_univdorm=svr_model(datas_univdorm,hours,data_partition)\n",
        "svr_time_univdorm=time.time() - start_time\n",
        "print(\"--- %s seconds - Support Vector Regression- univdorm ---\" % (svr_time_univdorm))\n",
        "\n",
        "\n",
        "#ANN\n",
        "start_time = time.time()\n",
        "ann_univdorm=ann_model(datas_univdorm,hours,data_partition)\n",
        "ann_time_univdorm=time.time() - start_time\n",
        "print(\"--- %s seconds - ANN- univdorm ---\" % (ann_time_univdorm))\n",
        "\n",
        "#random forest\n",
        "start_time = time.time()\n",
        "rf_univdorm=rf_model(datas_univdorm,hours,data_partition,max_features)\n",
        "rf_time_univdorm=time.time() - start_time\n",
        "print(\"--- %s seconds - Random Forest- univdorm ---\" % (rf_time_univdorm))\n",
        "\n",
        "#LSTM\n",
        "start_time = time.time()\n",
        "lstm_univdorm=lstm_model(datas_univdorm,hours,data_partition,max_features,epoch,batch_size,neuron,lr,optimizer)\n",
        "lstm_time_univdorm=time.time() - start_time\n",
        "print(\"--- %s seconds - lstm- univdorm ---\" % (lstm_time_univdorm))\n",
        "\n",
        "\n",
        "#CEEMDAN RF\n",
        "start_time = time.time()\n",
        "ceemdan_rf_univdorm=hybrid_ceemdan_rf(dfs_univdorm,hours,data_partition,max_features)\n",
        "ceemdan_rf_time_univdorm=time.time() - start_time\n",
        "print(\"--- %s seconds - ceemdan_rf- univdorm ---\" % (ceemdan_rf_time_univdorm))\n",
        "\n",
        "#CEEMDAN LSTM\n",
        "start_time = time.time()\n",
        "ceemdan_lstm_univdorm=hybrid_ceemdan_lstm(dfs_univdorm,hours,data_partition,max_features,epoch,batch_size,neuron,lr,optimizer)\n",
        "ceemdan_lstm_time_univdorm=time.time() - start_time\n",
        "print(\"--- %s seconds - ceemdan_lstm- univdorm ---\" % (ceemdan_lstm_time_univdorm))\n",
        "\n",
        "\n",
        "#proposed method\n",
        "start_time = time.time()\n",
        "proposed_method_univdorm=proposed_method(dfs_univdorm,hours,data_partition,max_features,epoch,batch_size,neuron,lr,optimizer)\n",
        "proposed_method_time_univdorm=time.time() - start_time\n",
        "print(\"--- %s seconds - proposed_method- univdorm ---\" % (proposed_method_time_univdorm))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "943bnd1lfgnO"
      },
      "source": [
        "## Summarize of experimental results with running time\n",
        "### Run this following cell will summarize the result and generate output used in Section 4.4 (Table 3) for University Dormitory dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vcgjqhCAfgnO",
        "outputId": "6c377670-cc52-43d4-bd87-aa1433196032"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    LR       SVR        ANN        RF  \\\n",
              "university dormitory results                                            \n",
              "0                             6.096747  6.486764   6.719073  6.153492   \n",
              "1                             3.091409  3.386055   3.389476  3.194408   \n",
              "2                             2.401101  2.590120   2.640636  2.440198   \n",
              "0                             0.375187  1.036478  15.300046  3.610328   \n",
              "\n",
              "                                   LSTM  CEEMDAN RF  CEEMDAN LSTM  \\\n",
              "university dormitory results                                        \n",
              "0                              6.875478    3.950790      4.193038   \n",
              "1                              3.459772    2.004695      2.123581   \n",
              "2                              2.701438    1.545388      1.632780   \n",
              "0                             26.363741   33.782516    140.674359   \n",
              "\n",
              "                              Proposed Method  \n",
              "university dormitory results                   \n",
              "0                                    3.458893  \n",
              "1                                    1.752256  \n",
              "2                                    1.350860  \n",
              "0                                  128.839404  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-b9878282-b3a7-4c4c-869f-b121df1e6c82\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LR</th>\n",
              "      <th>SVR</th>\n",
              "      <th>ANN</th>\n",
              "      <th>RF</th>\n",
              "      <th>LSTM</th>\n",
              "      <th>CEEMDAN RF</th>\n",
              "      <th>CEEMDAN LSTM</th>\n",
              "      <th>Proposed Method</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>university dormitory results</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.096747</td>\n",
              "      <td>6.486764</td>\n",
              "      <td>6.719073</td>\n",
              "      <td>6.153492</td>\n",
              "      <td>6.875478</td>\n",
              "      <td>3.950790</td>\n",
              "      <td>4.193038</td>\n",
              "      <td>3.458893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.091409</td>\n",
              "      <td>3.386055</td>\n",
              "      <td>3.389476</td>\n",
              "      <td>3.194408</td>\n",
              "      <td>3.459772</td>\n",
              "      <td>2.004695</td>\n",
              "      <td>2.123581</td>\n",
              "      <td>1.752256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.401101</td>\n",
              "      <td>2.590120</td>\n",
              "      <td>2.640636</td>\n",
              "      <td>2.440198</td>\n",
              "      <td>2.701438</td>\n",
              "      <td>1.545388</td>\n",
              "      <td>1.632780</td>\n",
              "      <td>1.350860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.375187</td>\n",
              "      <td>1.036478</td>\n",
              "      <td>15.300046</td>\n",
              "      <td>3.610328</td>\n",
              "      <td>26.363741</td>\n",
              "      <td>33.782516</td>\n",
              "      <td>140.674359</td>\n",
              "      <td>128.839404</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b9878282-b3a7-4c4c-869f-b121df1e6c82')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-52cba5b9-f8ed-4a1e-9af0-40e12fc6d9aa\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-52cba5b9-f8ed-4a1e-9af0-40e12fc6d9aa')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-52cba5b9-f8ed-4a1e-9af0-40e12fc6d9aa button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b9878282-b3a7-4c4c-869f-b121df1e6c82 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b9878282-b3a7-4c4c-869f-b121df1e6c82');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "running_time_univdorm=pd.DataFrame([lr_time_univdorm,svr_time_univdorm,ann_time_univdorm,\n",
        "                                   rf_time_univdorm,lstm_time_univdorm,ceemdan_rf_time_univdorm,\n",
        "                                   ceemdan_lstm_time_univdorm,proposed_method_time_univdorm])\n",
        "running_time_univdorm=running_time_univdorm.T\n",
        "running_time_univdorm.columns=['LR','SVR','ANN','RF','LSTM','CEEMDAN RF','CEEMDAN LSTM','Proposed Method']\n",
        "proposed_method_univdorm_df=proposed_method_univdorm[0:3]\n",
        "result_univdorm=pd.DataFrame([lr_univdorm,svr_univdorm,ann_univdorm,rf_univdorm,lstm_univdorm,ceemdan_rf_univdorm,\n",
        "                    ceemdan_lstm_univdorm,proposed_method_univdorm_df])\n",
        "result_univdorm=result_univdorm.T\n",
        "result_univdorm.columns=['LR','SVR','ANN','RF','LSTM','CEEMDAN RF','CEEMDAN LSTM','Proposed Method']\n",
        "univdorm_summary=pd.concat([result_univdorm,running_time_univdorm],axis=0)\n",
        "\n",
        "univdorm_summary.set_axis(['MAPE(%)', 'RMSE','MAE','running time (s)'], axis='index')\n",
        "\n",
        "univdorm_summary.style.set_caption(\"University Dormitory Results\")\n",
        "index = univdorm_summary.index\n",
        "index.name = \"university dormitory results\"\n",
        "univdorm_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "L8exM3QKfgnO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "703827bd-84eb-4a28-b0a1-27a9f7c78170"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-99a91c5d4a2a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    export table to png\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#export table to png\n",
        "#dfi.export(univdorm_summary,\"univdorm_summary_table.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpeUgzM2fgnP"
      },
      "source": [
        "## Calculate percentage improvement\n",
        "### Run this following cell will calculate percentage improvement and generate output used in Section 4.4 (Table 4) for University Dormitory dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "z-nkBOLZfgnP",
        "outputId": "92f2022b-1091-4da8-81ab-9fe942776b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             Proposed Method vs.LR  \\\n",
              "Percentage Improvement university dormitory                          \n",
              "0                                                            43.27   \n",
              "1                                                            43.32   \n",
              "2                                                            43.74   \n",
              "\n",
              "                                             Proposed Method vs.SVR  \\\n",
              "Percentage Improvement university dormitory                           \n",
              "0                                                             46.68   \n",
              "1                                                             48.25   \n",
              "2                                                             47.85   \n",
              "\n",
              "                                              Proposed Method vs.ANN  \\\n",
              "Percentage Improvement university dormitory                            \n",
              "0                                                              48.52   \n",
              "1                                                              48.30   \n",
              "2                                                              48.84   \n",
              "\n",
              "                                             Proposed Method vs.RF  \\\n",
              "Percentage Improvement university dormitory                          \n",
              "0                                                            43.79   \n",
              "1                                                            45.15   \n",
              "2                                                            44.64   \n",
              "\n",
              "                                             Proposed Method vs.LSTM  \\\n",
              "Percentage Improvement university dormitory                            \n",
              "0                                                              49.69   \n",
              "1                                                              49.35   \n",
              "2                                                              49.99   \n",
              "\n",
              "                                             Proposed Method vs.CEEMDAN RF  \\\n",
              "Percentage Improvement university dormitory                                  \n",
              "0                                                                    12.45   \n",
              "1                                                                    12.59   \n",
              "2                                                                    12.59   \n",
              "\n",
              "                                             Proposed Method vs. CEEMDAN LSTM  \n",
              "Percentage Improvement university dormitory                                    \n",
              "0                                                                       17.51  \n",
              "1                                                                       17.49  \n",
              "2                                                                       17.27  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-c0d436dc-2e0e-491f-abbd-ff9bfe7e8992\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Proposed Method vs.LR</th>\n",
              "      <th>Proposed Method vs.SVR</th>\n",
              "      <th>Proposed Method vs.ANN</th>\n",
              "      <th>Proposed Method vs.RF</th>\n",
              "      <th>Proposed Method vs.LSTM</th>\n",
              "      <th>Proposed Method vs.CEEMDAN RF</th>\n",
              "      <th>Proposed Method vs. CEEMDAN LSTM</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Percentage Improvement university dormitory</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>43.27</td>\n",
              "      <td>46.68</td>\n",
              "      <td>48.52</td>\n",
              "      <td>43.79</td>\n",
              "      <td>49.69</td>\n",
              "      <td>12.45</td>\n",
              "      <td>17.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>43.32</td>\n",
              "      <td>48.25</td>\n",
              "      <td>48.30</td>\n",
              "      <td>45.15</td>\n",
              "      <td>49.35</td>\n",
              "      <td>12.59</td>\n",
              "      <td>17.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>43.74</td>\n",
              "      <td>47.85</td>\n",
              "      <td>48.84</td>\n",
              "      <td>44.64</td>\n",
              "      <td>49.99</td>\n",
              "      <td>12.59</td>\n",
              "      <td>17.27</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0d436dc-2e0e-491f-abbd-ff9bfe7e8992')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-f8982d4c-0e9f-48c9-b341-ec2883693cfc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f8982d4c-0e9f-48c9-b341-ec2883693cfc')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-f8982d4c-0e9f-48c9-b341-ec2883693cfc button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c0d436dc-2e0e-491f-abbd-ff9bfe7e8992 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c0d436dc-2e0e-491f-abbd-ff9bfe7e8992');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "pMAPE_LR_vs_Proposed_univdorm=((lr_univdorm[0]-proposed_method_univdorm[0])/lr_univdorm[0])*100\n",
        "pRMSE_LR_vs_Proposed_univdorm=((lr_univdorm[1]-proposed_method_univdorm[1])/lr_univdorm[1])*100\n",
        "pMAE_LR_vs_Proposed_univdorm=((lr_univdorm[2]-proposed_method_univdorm[2])/lr_univdorm[2])*100\n",
        "\n",
        "pMAPE_SVR_vs_Proposed_univdorm=((svr_univdorm[0]-proposed_method_univdorm[0])/svr_univdorm[0])*100\n",
        "pRMSE_SVR_vs_Proposed_univdorm=((svr_univdorm[1]-proposed_method_univdorm[1])/svr_univdorm[1])*100\n",
        "pMAE_SVR_vs_Proposed_univdorm=((svr_univdorm[2]-proposed_method_univdorm[2])/svr_univdorm[2])*100\n",
        "\n",
        "pMAPE_ANN_vs_Proposed_univdorm=((ann_univdorm[0]-proposed_method_univdorm[0])/ann_univdorm[0])*100\n",
        "pRMSE_ANN_vs_Proposed_univdorm=((ann_univdorm[1]-proposed_method_univdorm[1])/ann_univdorm[1])*100\n",
        "pMAE_ANN_vs_Proposed_univdorm=((ann_univdorm[2]-proposed_method_univdorm[2])/ann_univdorm[2])*100\n",
        "\n",
        "pMAPE_RF_vs_Proposed_univdorm=((rf_univdorm[0]-proposed_method_univdorm[0])/rf_univdorm[0])*100\n",
        "pRMSE_RF_vs_Proposed_univdorm=((rf_univdorm[1]-proposed_method_univdorm[1])/rf_univdorm[1])*100\n",
        "pMAE_RF_vs_Proposed_univdorm=((rf_univdorm[2]-proposed_method_univdorm[2])/rf_univdorm[2])*100\n",
        "\n",
        "pMAPE_LSTM_vs_Proposed_univdorm=((lstm_univdorm[0]-proposed_method_univdorm[0])/lstm_univdorm[0])*100\n",
        "pRMSE_LSTM_vs_Proposed_univdorm=((lstm_univdorm[1]-proposed_method_univdorm[1])/lstm_univdorm[1])*100\n",
        "pMAE_LSTM_vs_Proposed_univdorm=((lstm_univdorm[2]-proposed_method_univdorm[2])/lstm_univdorm[2])*100\n",
        "\n",
        "pMAPE_ceemdan_rf_vs_Proposed_univdorm=((ceemdan_rf_univdorm[0]-proposed_method_univdorm[0])/ceemdan_rf_univdorm[0])*100\n",
        "pRMSE_ceemdan_rf_vs_Proposed_univdorm=((ceemdan_rf_univdorm[1]-proposed_method_univdorm[1])/ceemdan_rf_univdorm[1])*100\n",
        "pMAE_ceemdan_rf_vs_Proposed_univdorm=((ceemdan_rf_univdorm[2]-proposed_method_univdorm[2])/ceemdan_rf_univdorm[2])*100\n",
        "\n",
        "\n",
        "pMAPE_ceemdan_lstm_vs_Proposed_univdorm=((ceemdan_lstm_univdorm[0]-proposed_method_univdorm[0])/ceemdan_lstm_univdorm[0])*100\n",
        "pRMSE_ceemdan_lstm_vs_Proposed_univdorm=((ceemdan_lstm_univdorm[1]-proposed_method_univdorm[1])/ceemdan_lstm_univdorm[1])*100\n",
        "pMAE_ceemdan_lstm_vs_Proposed_univdorm=((ceemdan_lstm_univdorm[2]-proposed_method_univdorm[2])/ceemdan_lstm_univdorm[2])*100\n",
        "\n",
        "\n",
        "df_PI_univdorm=[[pMAPE_LR_vs_Proposed_univdorm,pMAPE_SVR_vs_Proposed_univdorm,pMAPE_ANN_vs_Proposed_univdorm,\n",
        "                pMAPE_RF_vs_Proposed_univdorm,pMAPE_LSTM_vs_Proposed_univdorm,pMAPE_ceemdan_rf_vs_Proposed_univdorm,\n",
        "                pMAPE_ceemdan_lstm_vs_Proposed_univdorm],\n",
        "                [pRMSE_LR_vs_Proposed_univdorm,pRMSE_SVR_vs_Proposed_univdorm,pRMSE_ANN_vs_Proposed_univdorm,\n",
        "                pRMSE_RF_vs_Proposed_univdorm,pRMSE_LSTM_vs_Proposed_univdorm,pRMSE_ceemdan_rf_vs_Proposed_univdorm,\n",
        "                pRMSE_ceemdan_lstm_vs_Proposed_univdorm],\n",
        "                [pMAE_LR_vs_Proposed_univdorm,pMAE_SVR_vs_Proposed_univdorm,pMAE_ANN_vs_Proposed_univdorm,\n",
        "                pMAE_RF_vs_Proposed_univdorm,pMAE_LSTM_vs_Proposed_univdorm,pMAE_ceemdan_rf_vs_Proposed_univdorm,\n",
        "                pMAE_ceemdan_lstm_vs_Proposed_univdorm]]\n",
        "\n",
        "PI_univdorm=pd.DataFrame(df_PI_univdorm, columns=[\"Proposed Method vs.LR\", \"Proposed Method vs.SVR\",\" Proposed Method vs.ANN\",\n",
        "                                      \"Proposed Method vs.RF\",\"Proposed Method vs.LSTM\",\"Proposed Method vs.CEEMDAN RF\",\n",
        "                                      \"Proposed Method vs. CEEMDAN LSTM\"])\n",
        "PI_univdorm= PI_univdorm.round(decimals = 2)\n",
        "PI_univdorm.set_axis(['MAPE(%)', 'RMSE','MAE'], axis='index')\n",
        "PI_univdorm.style.set_caption(\"Percentage Improvement-University Dormitory Building\")\n",
        "index = PI_univdorm.index\n",
        "index.name = \"Percentage Improvement university dormitory\"\n",
        "PI_univdorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpRbTb_WfgnP"
      },
      "outputs": [],
      "source": [
        "#export table to png\n",
        "#dfi.export(PI_univdorm,\"PI_univdorm_table.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKgyjEvXfgnP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}